{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "222번.카이의김씨들(김기영,김주희,김재훈).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGxU_ZS8DEE"
      },
      "source": [
        "\"\"\"commented out for test execution\"\"\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEZSFwyP5fQ"
      },
      "source": [
        "#공용함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUkWw1txP4XA"
      },
      "source": [
        "# 수정금지: 타임스탬프용 함수\n",
        "from datetime import datetime\n",
        "def printt(*args,**kwargs):\n",
        "  now = datetime.now()\n",
        "  now_str = \"{:02}:{:02}:{:02}\".format(now.hour,now.minute,now.second)\n",
        "  print(now_str, *args,**kwargs)\n",
        "  return int(now.hour)*60*60+int(now.minute)*60+int(now.second)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-h1MVpOSRVl"
      },
      "source": [
        "#연관 패키지 설치 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOHw8KJbRC4m"
      },
      "source": [
        "#TODO: 해당 블럭에 패키지 설치하세요.\n",
        "!pip install attrdict\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "!pip install fastprogress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIMuPDLIo0rb"
      },
      "source": [
        "# 파일로딩 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ4zJTn7o0LO"
      },
      "source": [
        "#TODO: 해당 블럭에 필요 파일 로딩 코드 넣으시오.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTVIEOA0Swkd"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOoJjT8Sx3R"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_start_time = printt(\"Model building: Start\")\n",
        "_model_build_start_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxnQUVccSa-V"
      },
      "source": [
        "#TODO: 블럭에 모델 학습 - 빌딩 코드를 넣으세요. (시간측정 구간)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIUoYKVugaUk"
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVLP6QJlgfuP"
      },
      "source": [
        "# 실행 전 런타임 다시시작\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "plt.plot([1, 2, 3, 4])\n",
        "plt.ylabel('한국어 테스트')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qi3tiTrV5Gy"
      },
      "source": [
        "import os\n",
        "!git clone https://github.com/Jaehun-Kim22/SCIC_Project.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN4qSwp3_2P8"
      },
      "source": [
        "# set config args for classification\n",
        "from transformers import (\n",
        "    ElectraConfig,\n",
        "    ElectraTokenizer,\n",
        "    ElectraForSequenceClassification,\n",
        "    XLMRobertaForSequenceClassification,\n",
        "    XLMRobertaTokenizer,\n",
        "    XLMRobertaConfig\n",
        ")\n",
        "from attrdict import AttrDict\n",
        "CLASSIFICATION_LIST = (\n",
        "    ['중립', '상담원', '상담시스템', '혜택', '할부금융상품', \n",
        "     '카드상품', '청구입금', '심사/한도', '생활편의서비스', '상담/채널',\n",
        "     '리스렌탈상품','라이프서비스', '금융상품', '고객정보관리', \n",
        "     '삼성카드', '기타']\n",
        ")\n",
        "CLASSIFICATION_DICT = {}\n",
        "for idx, item in enumerate(CLASSIFICATION_LIST):\n",
        "    CLASSIFICATION_DICT[str(idx)] = item\n",
        "\n",
        "args = AttrDict(\n",
        "    {\n",
        "        'version': 1,\n",
        "        'data_dir': 'SCIC_Project/assets/v1',\n",
        "        'train_file': 'train.txt',\n",
        "        'test_file': 'validation.txt',\n",
        "        'val_file': 'test.txt',\n",
        "        'task': 'classification',\n",
        "        'config': ElectraConfig,\n",
        "        'tokenizer': ElectraTokenizer,\n",
        "        'model': ElectraForSequenceClassification,\n",
        "        'evaluate_test_during_training': True, \n",
        "        'eval_all_checkpoints': True, \n",
        "        'save_optimizer': False, \n",
        "        'do_lower_case': False, \n",
        "        'do_train': True, \n",
        "        'do_eval': True, \n",
        "        'max_seq_len': 64, \n",
        "        'num_train_epochs': 10, \n",
        "        'num_train_epochs': 1, \n",
        "        'weight_decay': 0.0, \n",
        "        'gradient_accumulation_steps': 1, \n",
        "        'adam_epsilon': 1e-08, \n",
        "        'warmup_proportion': 0, \n",
        "        'max_steps': -1, \n",
        "        'max_grad_norm': 1.0, \n",
        "        'no_cuda': False, \n",
        "        'model_type': 'koelectra-base-v3', \n",
        "        'model_name_or_path': 'monologg/koelectra-base-v3-discriminator', \n",
        "        'output_dir': 'checkpoints/version_1', \n",
        "        'seed': 42, \n",
        "        'train_batch_size': 32, \n",
        "        'eval_batch_size': 32, \n",
        "        'logging_steps': 219, \n",
        "        'save_steps': 219, \n",
        "        'learning_rate': 5e-05,\n",
        "        'tb_root': './runs'\n",
        "     }\n",
        ")\n",
        "args.data_dir = os.path.join(args.data_dir)\n",
        "args.output_dir = args.task + args.output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJI_EzPkISd9"
      },
      "source": [
        "# initialize logging\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format= \"%(asctime)s - %(message)s\",\n",
        "    # format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUZThiu_893"
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# set seed\n",
        "torch.cuda.manual_seed_all(args.seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7j75Y0oGasc"
      },
      "source": [
        "# make Input Example\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, review_token, label1, label2):\n",
        "        self.review_token = review_token\n",
        "        self.label1 = label1\n",
        "        self.label2 = label2\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "        \n",
        "# make Input Feature\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label1, label2):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label1 = label1\n",
        "        self.label2 = label2\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyPH-5ErasPI"
      },
      "source": [
        "if args.task == 'sentiment':\n",
        "    args.logging_steps, args.save_steps = 63, 63\n",
        "    args.max_seq_len = 64\n",
        "    args.train_batch_size = 128\n",
        "    args.learning_rate = 2e-06\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def load_dataset(args, tokenizer, mode):\n",
        "    features = get_features(args, tokenizer, mode)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor(\n",
        "        [f.input_ids for f in features], \n",
        "        dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor(\n",
        "        [f.attention_mask for f in features],\n",
        "         dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor(\n",
        "        [f.token_type_ids for f in features],\n",
        "        dtype=torch.long)\n",
        "\n",
        "    all_labels_1 = torch.tensor([f.label1 for f in features],\n",
        "                        dtype=torch.float)\n",
        "    all_labels_2 = torch.tensor([f.label2 for f in features],\n",
        "                              dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids,\n",
        "                            all_attention_mask,\n",
        "                            all_token_type_ids,\n",
        "                            all_labels_1,\n",
        "                            all_labels_2)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_features(args, tokenizer, mode):\n",
        "    # load data and labels using Processor\n",
        "    processor = Processor(args)\n",
        "    try:\n",
        "        examples = processor.get_examples(mode)\n",
        "    except ValueError:\n",
        "        print('possible modes: train, val, test')\n",
        "    label_list = processor.get_labels()\n",
        "\n",
        "    # if args.task == 'classification':\n",
        "    labels1 = [float(example.label1) for example in examples]\n",
        "    labels2 = [int(example.label2) for example in examples]\n",
        "\n",
        "\n",
        "    batch_encoding = tokenizer.batch_encode_plus(\n",
        "        [(example.review_token) for example in examples],\n",
        "        max_length=args.max_seq_len,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    for i in range(len(examples)):\n",
        "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "        \n",
        "        # For xlm-roberta\n",
        "        inputs[\"token_type_ids\"] = [0] * len(inputs[\"input_ids\"])  \n",
        "\n",
        "        feature = InputFeatures(**inputs, label1=labels1[i], label2=labels2[i])\n",
        "        features.append(feature)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "class Processor(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "    def get_labels(self):\n",
        "        if self.args.task == 'classification':\n",
        "            return CLASSIFICATION_DICT.keys()\n",
        "        else:\n",
        "            return [None]\n",
        "\n",
        "    def _read_file(cls, input_file):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = []\n",
        "            for line in f:\n",
        "                lines.append(line.strip())\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines[0:]):\n",
        "            line = line.split(\"\\t\")\n",
        "            review_token = line[0]\n",
        "            label1 = line[1]\n",
        "            label2 = line[2]\n",
        "            examples.append(InputExample(review_token=review_token, label1=label1\n",
        "                                         , label2=label2))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, test\n",
        "        \"\"\"\n",
        "        file_to_read = None\n",
        "        if mode == \"train\":\n",
        "            file_to_read = self.args.train_file\n",
        "        elif mode == \"test\":\n",
        "            file_to_read = self.args.test_file\n",
        "        elif mode == 'val':\n",
        "            file_to_read = self.args.val_file\n",
        "\n",
        "        return self._create_examples(\n",
        "            self._read_file(os.path.join(self.args.data_dir, file_to_read)), mode\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwf-8cQ9EpgT"
      },
      "source": [
        "tokenizer = args.tokenizer.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    do_lower_case=args.do_lower_case\n",
        ")\n",
        "\n",
        "train_dataset = load_dataset(args, tokenizer, mode=\"train\")\n",
        "val_dataset = load_dataset(args, tokenizer, mode=\"test\")\n",
        "datasets = {'train': train_dataset, 'val': val_dataset}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VARfUcFjGPam"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IYdUfyLg82i"
      },
      "source": [
        "%tensorboard --logdir=args.tb_root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZXbz67inCgG"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "import torch\n",
        "\n",
        "# TODO: acc. calculating logic impl. + figure adding logic impl.\n",
        "        \n",
        "def train(args, datasets):\n",
        "    train_sampler = RandomSampler(datasets['train'])\n",
        "    train_loader = DataLoader(datasets['train'],\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=args.train_batch_size)\n",
        "    val_sampler = RandomSampler(datasets['val'])\n",
        "    val_loader = DataLoader(datasets['val'],\n",
        "                            sampler=val_sampler,\n",
        "                            batch_size=args.eval_batch_size)\n",
        "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "    full_text = pd.read_csv(os.path.join(args.data_dir, 'full_text.txt'),\n",
        "                            sep='\\t')\n",
        "    \n",
        "    t_total = (\n",
        "        len(dataloaders['train']) // args.gradient_accumulation_steps\n",
        "        * args.num_train_epochs\n",
        "    )\n",
        "\n",
        "    writer = SummaryWriter(\n",
        "        os.path.join(args.tb_root, f'version_{args.version}')\n",
        "        )\n",
        "\n",
        "    # Load models <model_1: sentiment, model_2: classifiation>\n",
        "    config_1 = args.config.from_pretrained(args.model_name_or_path,\n",
        "                                           num_labels=1)\n",
        "    model_1 = args.model.from_pretrained(args.model_name_or_path,\n",
        "                                         config=config_1).to(args.device)\n",
        "    config_2 = args.config.from_pretrained(args.model_name_or_path,\n",
        "                                           num_labels=16)\n",
        "    model_2 = args.model.from_pretrained(args.model_name_or_path,\n",
        "                                         config=config_2).to(args.device)\n",
        "\n",
        "   \n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters_1 = [\n",
        "        {\n",
        "            'params': [p for n, p in model_1.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': args.weight_decay\n",
        "         },\n",
        "        {\n",
        "            'params': [p for n, p in model_1.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0\n",
        "         }\n",
        "    ]\n",
        "    optimizer_grouped_parameters_2 = [\n",
        "        {\n",
        "            'params': [p for n, p in model_2.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': args.weight_decay\n",
        "         },\n",
        "        {\n",
        "            'params': [p for n, p in model_2.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0\n",
        "         }\n",
        "    ]\n",
        "    optimizer_1 = AdamW(optimizer_grouped_parameters_1,\n",
        "                      lr=5e-06,\n",
        "                      eps=args.adam_epsilon)\n",
        "    optimizer_2 = AdamW(optimizer_grouped_parameters_2,\n",
        "                      lr=args.learning_rate,\n",
        "                      eps=args.adam_epsilon)\n",
        "    scheduler_1 = get_linear_schedule_with_warmup(\n",
        "        optimizer_1,\n",
        "        num_warmup_steps=int(t_total * args.warmup_proportion),\n",
        "        num_training_steps=t_total\n",
        "        )\n",
        "    scheduler_2 = get_linear_schedule_with_warmup(\n",
        "        optimizer_2,\n",
        "        num_warmup_steps=int(t_total * args.warmup_proportion),\n",
        "        num_training_steps=t_total\n",
        "        )\n",
        "\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) \\\n",
        "    and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\")):\n",
        "        # Load optimizer and scheduler states\n",
        "        optimizer_1.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "            )\n",
        "        optimizer_2.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "            )\n",
        "        scheduler_1.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "            )\n",
        "        scheduler_2.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "            )\n",
        "\n",
        "    print(f'training started <model_1: sentiment, model_2: classifiation>')\n",
        "    global_step = 0\n",
        "    model_1.zero_grad()\n",
        "    model_2.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    for epoch in mb:\n",
        "        mb.main_bar.comment = f'training in progress: epoch {epoch+1}'\n",
        "        accuracies_1, accuracies_2, losses = {}, {}, {}\n",
        "        for stage, dataloader in dataloaders.items():\n",
        "            epoch_iterator = progress_bar(dataloader, parent=mb)\n",
        "            num_correct_1, num_correct_2, num_items_1, num_items_2 = 0, 0, 0, 0\n",
        "            running_loss = 0\n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                mb.child.comment = f'epoch {epoch+1} {stage}'\n",
        "                if stage == 'train':\n",
        "                    model_1.train()\n",
        "                    model_2.train()\n",
        "                    optimizer_1.zero_grad() # why is this not in original func?\n",
        "                    optimizer_2.zero_grad()\n",
        "                else:\n",
        "                    model_1.eval()\n",
        "                    model_2.eval()\n",
        "                \n",
        "                batch = tuple(t.to(args.device) for t in batch)\n",
        "                \n",
        "                inputs_1 = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"token_type_ids\": batch[2],\n",
        "                    \"labels\": batch[3]\n",
        "                }\n",
        "                # model output: [loss, grad_fn (MSE), logits, batches]\n",
        "                outputs_1 = model_1(**inputs_1)\n",
        "                preds_1 = torch.round(outputs_1.logits).view(-1)\n",
        "                num_items_1 += preds_1.shape[0]\n",
        "                num_correct_1 += (inputs_1['labels'] == preds_1).sum()\n",
        "                loss_1 = outputs_1[0]\n",
        "\n",
        "                inputs_2 = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"token_type_ids\": batch[2],\n",
        "                    \"labels\": batch[4]\n",
        "                }\n",
        "                # model output: [loss, grad_fn (NLL), logits, batches]\n",
        "                outputs_2 = model_2(**inputs_2)\n",
        "                preds_2 = torch.argmax(outputs_2.logits, dim=1)\n",
        "                num_items_2 += preds_1.shape[0]\n",
        "                num_correct_2 += (inputs_2['labels'] == preds_2).sum()\n",
        "                loss_2 = outputs_2[0]\n",
        "\n",
        "                # loss combination + model backward\n",
        "                loss_1, loss_2 = loss_1 + loss_2, loss_1 + loss_2\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "                if stage == 'train':\n",
        "                    loss_1.backward(retain_graph=True)\n",
        "                    loss_2.backward()\n",
        "                running_loss += loss_1.item()\n",
        "\n",
        "                if stage == 'train' and (\n",
        "                    (step + 1) % args.gradient_accumulation_steps == 0 \n",
        "                    or (len(dataloader) <= args.gradient_accumulation_steps\n",
        "                        and (step + 1) == len(dataloader)\n",
        "                        )\n",
        "                    ):\n",
        "                    torch.nn.utils.clip_grad_norm_(model_1.parameters(),\n",
        "                                                   args.max_grad_norm)\n",
        "                    torch.nn.utils.clip_grad_norm_(model_2.parameters(),\n",
        "                                                   args.max_grad_norm)\n",
        "\n",
        "                    optimizer_1.step()\n",
        "                    optimizer_2.step()\n",
        "                    scheduler_1.step()\n",
        "                    scheduler_2.step()\n",
        "                    model_1.zero_grad()\n",
        "                    model_2.zero_grad()\n",
        "                    global_step += 1\n",
        "                if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                    break\n",
        "            accuracies_1[stage] = num_correct_1 / num_items_1\n",
        "            accuracies_2[stage] = num_correct_2 / num_items_2\n",
        "            losses[stage] = running_loss / len(dataloader)\n",
        "        # Save model checkpoint\n",
        "        output_dir_1 = os.path.join(args.output_dir,\n",
        "                                    \"1-checkpoint-{}\".format(epoch + 1))\n",
        "        output_dir_2 = os.path.join(args.output_dir,\n",
        "                                    \"2-checkpoint-{}\".format(epoch + 1))\n",
        "        if not os.path.exists(output_dir_1):\n",
        "            os.makedirs(output_dir_1)\n",
        "        if not os.path.exists(output_dir_2):\n",
        "            os.makedirs(output_dir_2)\n",
        "        model_1_to_save = (\n",
        "            model_1.module if hasattr(model_1, \"module\") else model_1\n",
        "        )\n",
        "        model_1_to_save.save_pretrained(output_dir_1)\n",
        "        model_2_to_save = (\n",
        "            model_2.module if hasattr(model_2, \"module\") else model_2\n",
        "        )\n",
        "        model_2_to_save.save_pretrained(output_dir_2)\n",
        "\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "        \n",
        "        # Tensorboard scalar logging\n",
        "        writer.add_scalars(main_tag='accuracy_1',\n",
        "                           tag_scalar_dict=accuracies_1,\n",
        "                           global_step=epoch+1)\n",
        "        writer.add_scalars(main_tag='accuracy_2',\n",
        "                           tag_scalar_dict=accuracies_2,\n",
        "                           global_step=epoch+1)        \n",
        "        writer.add_scalars(main_tag='loss',\n",
        "                          tag_scalar_dict=losses,\n",
        "                          global_step=epoch+1)\n",
        "        # print(accuracies_1, accuracies_2)\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            del model_1, model_2\n",
        "            torch.cuda.empty_cache()\n",
        "            break\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-5E4fNnbURq"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "torch.cuda.memory_summary(abbreviated=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxQ2yD-kN-H"
      },
      "source": [
        "# GPU or CPU\n",
        "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "\n",
        "\n",
        "# Train\n",
        "train(args, datasets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYvSYeuUekee"
      },
      "source": [
        "print(args.output_dir + \"/1-checkpoint-1/**/\" + \"pytorch_model.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqGYKx2Lw286"
      },
      "source": [
        "test_dataset = load_dataset(args, tokenizer, mode=\"val\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYa33-cXnym-"
      },
      "source": [
        "def test(args, test_data, mode, global_step=None):\n",
        "\n",
        "  \n",
        "  checkpoints_1 = list(\n",
        "              os.path.dirname(c) for c in\n",
        "              sorted(glob.glob(args.output_dir + \"/1-checkpoint-1/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "          )\n",
        "  checkpoints_2 = list(\n",
        "              os.path.dirname(c) for c in\n",
        "              sorted(glob.glob( args.output_dir + \"/2-checkpoint-1/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "          )\n",
        "  print(checkpoints_1, checkpoints_2)\n",
        "  checkpoint_1 = checkpoints_1[-1]\n",
        "  checkpoint_2 = checkpoints_2[-1]\n",
        "  # logger.info(\"Test the following checkpoint: %s\", checkpoint)\n",
        "  # global_step = checkpoint_1.split(\"-\")[-1]\n",
        "\n",
        "  model_1 = args.model.from_pretrained(checkpoint_1).to(args.device)\n",
        "  model_2 = args.model.from_pretrained(checkpoint_2).to(args.device)\n",
        "\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args.eval_batch_size)\n",
        "  num_correct_1, num_correct_2, num_items_1, num_items_2 = 0, 0, 0, 0\n",
        "  df_result = pd.DataFrame(columns=['Review', 'Label1', 'Label2'])\n",
        "  for batch in progress_bar(test_dataloader):\n",
        "      preds_temp = None\n",
        "      model_1.eval()\n",
        "      model_2.eval()\n",
        "      batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          inputs_1 = {\n",
        "              \"input_ids\": batch[0],\n",
        "              \"attention_mask\": batch[1],\n",
        "              \"token_type_ids\": batch[2],\n",
        "              # label 없이 할 땐 여기 주석 반전\n",
        "              \"labels\": batch[3]\n",
        "          }\n",
        "          inputs_2 = {\n",
        "              \"input_ids\": batch[0],\n",
        "              \"attention_mask\": batch[1],\n",
        "              \"token_type_ids\": batch[2],\n",
        "              # label 없이 할 땐 여기 주석 반전\n",
        "              \"labels\": batch[4]\n",
        "          }          \n",
        "          outputs_1 = model_1(**inputs_1)\n",
        "          outputs_2 = model_2(**inputs_2)\n",
        "          preds_1 = torch.round(outputs_1.logits).view(-1)\n",
        "          preds_2 = torch.argmax(outputs_2.logits, dim=1)\n",
        "          num_items_1 += preds_1.shape[0]\n",
        "          check_preds_1 = inputs_1['labels'] == preds_1\n",
        "          check_preds_2 = inputs_2['labels'] == preds_2\n",
        "          num_correct_1 += (check_preds_1).sum()          \n",
        "          num_items_2 += preds_2.shape[0]\n",
        "          num_correct_2 += (check_preds_2).sum()\n",
        "        #   id_list = []\n",
        "        #   [id_list.append(' '.join([j for j in tokenizer.decode(i).split(' ') if not \n",
        "        #                   j in ('[PAD]', '[CLS]', '[SEP]')])) for i in batch[0].detach().cpu().numpy()]\n",
        "        #   [id_list.append([j for j in tokenizer.decode(i).split(' ') if not \n",
        "        #                    j in ('[PAD]', '[CLS]', '[SEP]')])\n",
        "        #                     for i in batch[0].detach().cpu().numpy()]\n",
        "        #   print(id_list)\n",
        "        #   [print(tokenizer.decode(i)) for i in batch[0].detach().cpu().numpy()]\n",
        "          # df_temp = pd.DataFrame(list(batch[0].detach().cpu().numpy()), \n",
        "          #                        columns=['Review'])\n",
        "          # df_temp['Label1'] = preds_1.detach().cpu().numpy()\n",
        "          # df_temp['Label2'] = preds_2.detach().cpu().numpy()\n",
        "          # df_result.append(df.temp)\n",
        "  print(num_correct_1 / num_items_1, num_correct_2 / num_items_2)\n",
        "  # print(df_result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKjpWMZ3jR2X"
      },
      "source": [
        "test_result = test(args, test_dataset, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kj31GrlxBpV"
      },
      "source": [
        "list_preds = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDBbGwMkn_pa"
      },
      "source": [
        "list_preds.append(test_result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKkgQ6WTolBG"
      },
      "source": [
        "print(len(list_preds[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBDAIaPmo__K"
      },
      "source": [
        "def complete_result(list_preds):\n",
        "  label_dict_classification = {'중립': 0, '상담원': 1, '상담시스템': 2, '혜택': 3, '할부금융상품': 4,\n",
        "                '카드상품': 5, '청구입금': 6, '심사/한도': 7, '생활편의서비스': 8,\n",
        "                '상담/채널': 9, '리스렌탈상품': 10, '라이프서비스': 11, '금융상품': 12,\n",
        "                '고객정보관리': 13, '가맹점매출/승인': 14, '삼성카드': 15, '기타': 16}\n",
        "  label_dict_sentiment = {'칭찬': 0, '불만': 1}\n",
        "  label_dict_classification = dict((v, k) for k, v in label_dict_classification.items())\n",
        "  label_dict_sentiment = dict((v, k) for k, v in label_dict_sentiment.items())\n",
        "  for i in label_dict_classification:\n",
        "    if i not in [0, 1, 2, 16]:\n",
        "      label_dict_classification[i] = '>삼성카드>' + label_dict_classification[i]\n",
        "    else:\n",
        "      if i == (1 or 2):\n",
        "        label_dict_classification[i] = '>고객서비스>' + label_dict_classification[i]\n",
        "      if i == 16:\n",
        "        label_dict_classification[i] = '>' + label_dict_classification[i]\n",
        "  df = pd.DataFrame(columns = ['INT'])\n",
        "  sent_result = np.vectorize(label_dict_sentiment.get)(list_preds[0])\n",
        "  # class_result = np.vectorize(label_dict_classification.get)(list_preds[1])\n",
        "  # result_arr = np.core.defchararray.add(sent_result, class_result)\n",
        "  df['INT'] = sent_result\n",
        "  print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6iBwzHpDTU"
      },
      "source": [
        "complete_result(list_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOFBaYi8S7h6"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_end_time = printt(\"Model building: Start\")\n",
        "print(_model_build_end_time - _model_build_start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZX6jC22S99-"
      },
      "source": [
        "#모델 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPEMmgXfTKf9"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_start_time = printt(\"TEST: Start\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLDvBcGMSdwg"
      },
      "source": [
        "#TODO: 해당 블럭에 테스트 수행을 위한 코드를 넣으세요. (시간측정 구간)\n",
        "#분석 파일은 tsv 파일로 제공되며, 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 =공백)으로 제공됩니다.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLsLL-vSq1D"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_end_time = printt(\"Model building: Start\")\n",
        "print(_test_end_time - _test_start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_NifWnpKw-"
      },
      "source": [
        "# 결과출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UctiMXI8UQ0Q"
      },
      "source": [
        "#TODO:해당 블럭에 테스트 결과를 파일로 저장하는 코드를 넣으세요. (시간측정 제외)\n",
        "#저장 파일은tsv 파일로 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 = 테스트 결과 도출된 양식)으로 저장\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmpdIIPmkMez"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR-CGR8LIVQW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uYU_MBjIYQr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
