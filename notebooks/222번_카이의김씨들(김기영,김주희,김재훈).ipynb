{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "222번.카이의김씨들(김기영,김주희,김재훈).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e184ffbc3ed452e82c305237ba2fa66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f9f61f6bb827492b809810baf00761c6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_865583cbea934829badf857d7b79cceb",
              "IPY_MODEL_22bf8ed41a7047d2a5b317b575ed27f6"
            ]
          }
        },
        "f9f61f6bb827492b809810baf00761c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "865583cbea934829badf857d7b79cceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e61916f6dec240b8aa7fb14c393a291e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d480d2de88764092a2a6cec4afdf2362"
          }
        },
        "22bf8ed41a7047d2a5b317b575ed27f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_04362671776549569cf73a1381f9e08e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05010de22972430886d1c495933b93ad"
          }
        },
        "e61916f6dec240b8aa7fb14c393a291e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d480d2de88764092a2a6cec4afdf2362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04362671776549569cf73a1381f9e08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05010de22972430886d1c495933b93ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGxU_ZS8DEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee79b38-8328-4e10-cb93-0412eb2d70a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEZSFwyP5fQ"
      },
      "source": [
        "#공용함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUkWw1txP4XA"
      },
      "source": [
        "# 수정금지: 타임스탬프용 함수\n",
        "from datetime import datetime\n",
        "def printt(*args,**kwargs):\n",
        "  now = datetime.now()\n",
        "  now_str = \"{:02}:{:02}:{:02}\".format(now.hour,now.minute,now.second)\n",
        "  print(now_str, *args,**kwargs)\n",
        "  return int(now.hour)*60*60+int(now.minute)*60+int(now.second)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-h1MVpOSRVl"
      },
      "source": [
        "#연관 패키지 설치 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOHw8KJbRC4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffef285b-5802-4add-d046-43b17b679617"
      },
      "source": [
        "#TODO: 해당 블럭에 패키지 설치하세요.\n",
        "!pip install attrdict\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "!pip install fastprogress"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from attrdict) (1.15.0)\n",
            "Installing collected packages: attrdict\n",
            "Successfully installed attrdict-2.0.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 40.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=790786135f6d53ce58b99743f3348824f41066cfd29166deda350f2ba9e2e66e\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIMuPDLIo0rb"
      },
      "source": [
        "# 파일로딩 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ4zJTn7o0LO"
      },
      "source": [
        "#TODO: 해당 블럭에 필요 파일 로딩 코드 넣으시오.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTVIEOA0Swkd"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOoJjT8Sx3R"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_start_time = printt(\"Model building: Start\")\n",
        "_model_build_start_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxnQUVccSa-V"
      },
      "source": [
        "#TODO: 블럭에 모델 학습 - 빌딩 코드를 넣으세요. (시간측정 구간)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJI_EzPkISd9"
      },
      "source": [
        "# initialize logging\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaOFiXQm_xSk"
      },
      "source": [
        "# make Input Example\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b, label):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "        \n",
        "# make Input Feature\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "# make processor\n",
        "class Processor(object):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "    def get_labels(self):\n",
        "        # return ['None', '상담원', '상담시스템', '고객서비스', '혜택', '할부금융상품', '커뮤니티서비스',\n",
        "        #         '카드이용/결제', '카드상품', '청구입금', '심사/한도', '생활편의서비스', '상담/채널', '리스렌탈상품',\n",
        "        #         '라이프서비스', '금융상품', '고객정보관리', '가맹점매출/승인', '가맹점대금', '가맹점계약', '삼성카드', '기타']\n",
        "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
        "                '13', '14', '15', '16', '17', '18', '19', '20', '21']\n",
        "\n",
        "    def _read_file(cls, input_file):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = []\n",
        "            for line in f:\n",
        "                lines.append(line.strip())\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines[0:]):\n",
        "            line = line.split(\"\\t\")\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[0]\n",
        "            label = line[1]\n",
        "            if i % 10000 == 0:\n",
        "                logger.info(line)\n",
        "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, test\n",
        "        \"\"\"\n",
        "        file_to_read = None\n",
        "        if mode == \"train\":\n",
        "            file_to_read = self.args.train_file\n",
        "        elif mode == \"test\":\n",
        "            file_to_read = self.args.test_file\n",
        "\n",
        "        logger.info(\"LOOKING AT {}\".format(os.path.join(self.args.data_dir, file_to_read)))\n",
        "        return self._create_examples(\n",
        "            self._read_file(os.path.join(self.args.data_dir, file_to_read)), mode\n",
        "        )"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN4qSwp3_2P8"
      },
      "source": [
        "# set config args for classification\n",
        "from transformers import (\n",
        "    ElectraConfig,\n",
        "    ElectraTokenizer,\n",
        "    ElectraForSequenceClassification,\n",
        ")\n",
        "from attrdict import AttrDict\n",
        "args = AttrDict(\n",
        "    {\n",
        "                 'data_dir': 'data',\n",
        "                 'task': 'classification',\n",
        "                 'train_file': 'train_n.txt',\n",
        "                 'test_file': 'test_n.txt',\n",
        "                 'config': ElectraConfig,\n",
        "                 'tokenizer': ElectraTokenizer,\n",
        "                 'model': ElectraForSequenceClassification,\n",
        "                 'evaluate_test_during_training': True, \n",
        "                 'eval_all_checkpoints': True, \n",
        "                 'save_optimizer': False, \n",
        "                 'do_lower_case': False, \n",
        "                 'do_train': True, \n",
        "                 'do_eval': True, \n",
        "                 'max_seq_len': 128, \n",
        "                 'num_train_epochs': 1, \n",
        "                 'weight_decay': 0.0, \n",
        "                 'gradient_accumulation_steps': 1, \n",
        "                 'adam_epsilon': 1e-08, \n",
        "                 'warmup_proportion': 0, \n",
        "                 'max_steps': -1, \n",
        "                 'max_grad_norm': 1.0, \n",
        "                 'no_cuda': False, \n",
        "                 'model_type': 'koelectra-base-v3', \n",
        "                 'model_name_or_path': 'monologg/koelectra-base-v3-discriminator', \n",
        "                 'output_dir': 'koelectra-base-v3-nsmc-ckpt', \n",
        "                 'seed': 42, \n",
        "                 'train_batch_size': 32, \n",
        "                 'eval_batch_size': 32, \n",
        "                 'logging_steps': 1, \n",
        "                 'save_steps': 1, \n",
        "                 'learning_rate': 5e-05\n",
        "     }\n",
        ")"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEUZThiu_893",
        "outputId": "e87183bb-8e0b-4e09-b06e-1ba073730630"
      },
      "source": [
        "import torch\n",
        "\n",
        "# set seed\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "processor = Processor(args)\n",
        "labels = processor.get_labels()\n",
        "\n",
        "\n",
        "config = args.config.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            num_labels=len(labels)\n",
        "            # id2label={str(i): label for i, label in enumerate(labels)},\n",
        "            # label2id={label: i for i, label in enumerate(labels)},\n",
        "        )\n",
        "tokenizer = args.tokenizer.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    do_lower_case=args.do_lower_case\n",
        ")\n",
        "model = args.model.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    config=config\n",
        ")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfIJc-O3AAuH",
        "outputId": "c5664b70-f5a8-4ed6-bdf0-ff2d2595cdba"
      },
      "source": [
        "import os\n",
        "from torch.utils.data import TensorDataset\n",
        "logger = logging.getLogger(__name__)\n",
        "print(__name__)\n",
        "def load_and_cache_examples(args, tokenizer, mode):\n",
        "    processor = Processor(args)\n",
        "    output_mode = args.task\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cached_{}_{}_{}_{}\".format(\n",
        "            str(args.task), list(filter(None, args.model_name_or_path.split(\"/\"))).pop(), str(args.max_seq_len), mode\n",
        "        ),\n",
        "    )\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        if mode == \"train\":\n",
        "            examples = processor.get_examples(\"train\")\n",
        "        elif mode == \"test\":\n",
        "            examples = processor.get_examples(\"test\")\n",
        "        else:\n",
        "            raise ValueError(\"For mode, choose between train and test\")\n",
        "        features = seq_cls_convert_examples_to_features(\n",
        "            args, examples, tokenizer, max_length=args.max_seq_len\n",
        "        )\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "    return dataset\n",
        "def seq_cls_convert_examples_to_features(args, examples, tokenizer, max_length):\n",
        "    processor = Processor(args)\n",
        "    label_list = processor.get_labels()\n",
        "    output_mode = 'classification'\n",
        "    logger.info(\"Using label list {}\".format(label_list))\n",
        "    logger.info(\"Using output mode {}\".format(output_mode))\n",
        "\n",
        "    # labels = [label_from_example(example) for example in examples]\n",
        "    labels = [int(example.label) for example in examples]\n",
        "\n",
        "    batch_encoding = tokenizer.batch_encode_plus(\n",
        "        [(example.text_a, example.text_b) for example in examples],\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    for i in range(len(examples)):\n",
        "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "        if \"token_type_ids\" not in inputs:\n",
        "            inputs[\"token_type_ids\"] = [0] * len(inputs[\"input_ids\"])  # For xlm-roberta\n",
        "\n",
        "        feature = InputFeatures(**inputs, label=labels[i])\n",
        "        features.append(feature)\n",
        "\n",
        "    for i, example in enumerate(examples[:5]):\n",
        "        logger.info(\"*** Example ***\")\n",
        "        logger.info(\"guid: {}\".format(example.guid))\n",
        "        logger.info(\"input_ids: {}\".format(\" \".join([str(x) for x in features[i].input_ids])))\n",
        "        logger.info(\"attention_mask: {}\".format(\" \".join([str(x) for x in features[i].attention_mask])))\n",
        "        logger.info(\"token_type_ids: {}\".format(\" \".join([str(x) for x in features[i].token_type_ids])))\n",
        "        logger.info(\"label: {}\".format(features[i].label))\n",
        "\n",
        "    return features\n",
        "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
        "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2021 09:08:34 - INFO - __main__ -   Creating features from dataset file at ./data\n",
            "08/05/2021 09:08:34 - INFO - __main__ -   LOOKING AT ./data/train_n.txt\n",
            "08/05/2021 09:08:34 - INFO - __main__ -   ['다른 카드사보다 명세서가 많이 늦습니다  11일 결제일인데 적어도 5일 이내로 도착할 수 있게 신경 써주심 감사하겠습니다', '9']\n",
            "08/05/2021 09:08:34 - INFO - __main__ -   Using label list ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21']\n",
            "08/05/2021 09:08:34 - INFO - __main__ -   Using output mode classification\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__main__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "08/05/2021 09:08:36 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   guid: train-0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   input_ids: 2 6254 6747 4067 4275 4176 26167 4129 4070 6341 2336 4576 6216 6307 4366 8567 4366 4139 4244 9282 25 4366 8497 4239 7520 4519 2967 3249 4325 7340 3034 4076 4437 6851 4279 5012 4576 6216 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   label: 9\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   guid: train-1\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   input_ids: 2 10313 4200 4005 4309 3063 18781 3178 10792 4070 2702 4070 6289 4279 4761 10792 4110 7855 4076 4820 27233 10313 4200 4005 4309 4311 6364 4073 4129 3325 9678 3063 18781 10792 3123 7218 6353 4086 6267 4007 2684 3755 4034 2054 6619 4070 3081 4070 4116 4150 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   label: 11\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   guid: train-2\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   input_ids: 2 8131 6267 4073 4129 8888 4279 4147 7836 6750 10749 6686 4112 3755 4737 17164 2990 4325 19468 2967 3249 26147 3429 4292 6284 4025 3323 5240 9234 3311 5012 4576 6216 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   label: 12\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   guid: train-3\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   input_ids: 2 3098 6267 4292 6493 31397 4176 2780 4155 13408 4021 4239 3760 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   label: 12\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   guid: train-4\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   input_ids: 2 2633 4112 10342 9880 4007 20625 4219 3249 4065 6465 6289 17788 6851 17788 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   label: 1\n",
            "08/05/2021 09:08:36 - INFO - __main__ -   Saving features into cached file ./data/cached_classification_koelectra-base-v3-discriminator_128_train\n",
            "08/05/2021 09:08:37 - INFO - __main__ -   Creating features from dataset file at ./data\n",
            "08/05/2021 09:08:37 - INFO - __main__ -   LOOKING AT ./data/test_n.txt\n",
            "08/05/2021 09:08:37 - INFO - __main__ -   ['마케팅이라는  부서에서 앞으로는  절대  전화 사절합니다  다시 한번 전하는데 절대  반드시 전화 사절합니다   본사 직원에게도 분명히  이야기했으니 절대로 전화 사절합니다', '16']\n",
            "08/05/2021 09:08:37 - INFO - __main__ -   Using label list ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21']\n",
            "08/05/2021 09:08:37 - INFO - __main__ -   Using output mode classification\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   guid: test-0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   input_ids: 2 7811 4007 30524 8599 4073 4129 3092 10749 4034 7546 6698 18086 17788 6304 3757 4467 9358 18781 7546 7433 6698 18086 17788 9266 6675 4073 4325 4086 8370 6402 4398 8762 11641 6698 18086 17788 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   label: 16\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   guid: test-1\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   input_ids: 2 6901 4289 7307 6699 4292 28161 2468 12546 7163 4110 3305 10052 4076 8553 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   label: 1\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   guid: test-2\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   input_ids: 2 23335 3098 6395 8452 4151 4150 13716 3764 4076 8553 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   label: 12\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   guid: test-3\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   input_ids: 2 7864 4067 4473 4007 18867 18565 18565 11029 4279 22078 6851 17788 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   label: 1\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   *** Example ***\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   guid: test-4\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   input_ids: 2 14573 2631 4154 8553 28588 4065 8911 29365 4219 6652 4279 8553 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   label: 21\n",
            "08/05/2021 09:08:38 - INFO - __main__ -   Saving features into cached file ./data/cached_classification_koelectra-base-v3-discriminator_128_test\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZXbz67inCgG"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def train(args,\n",
        "          model,\n",
        "          train_dataset,\n",
        "          test_dataset=None):\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "    full_text = pd.read_csv(os.path.join(args.data_dir, 'train_n_full.txt'), sep='\\t')\n",
        "    # full_text = pd.read_csv('train_n_full.txt', sep='\\t')\n",
        "    print(full_text)\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        print()\n",
        "        print(t_total, len(train_dataloader), args.gradient_accumulation_steps)\n",
        "        print()\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "        print('max steps: ' + str(t_total), 'length of train data: ' + str(len(train_dataloader)),\n",
        "              args.gradient_accumulation_steps)\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(t_total * args.warmup_proportion),\n",
        "                                                num_training_steps=t_total)\n",
        "\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Total train batch size = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "    logger.info(\"  Logging steps = %d\", args.logging_steps)\n",
        "    logger.info(\"  Save steps = %d\", args.save_steps)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss = 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    # for epoch in tqdm(range(int(args.num_train_epochs))):\n",
        "    #     sleep(0.1)\n",
        "    # len_8 = 0\n",
        "    # len_16 = 0\n",
        "    # len_32 = 0\n",
        "    # len_64 = 0\n",
        "    # len_128 = 0\n",
        "    # len_256 = 0\n",
        "    # len_512 = 0\n",
        "    for epoch in tqdm(mb):\n",
        "        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
        "\n",
        "        # 내가 수정한 부분\n",
        "        # with tqdm(total=t_total / args.num_train_epochs ) as pbar:\n",
        "            #\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            #\n",
        "            # print(len(batch))\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"labels\": batch[3]\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "            # 데이터 길이 분석\n",
        "            # logits = batch[0]\n",
        "            # temp = logits.detach().cpu().numpy()\n",
        "            # for i in range(len(temp)):\n",
        "            #     # print(i)\n",
        "            #     review_list = list(temp[i])\n",
        "            #     while 0 in review_list:\n",
        "            #         review_list.remove(0)\n",
        "            #     if len(review_list) < 256:\n",
        "            #         len_256 += 1\n",
        "            #     if len(review_list) < 128:\n",
        "            #         len_128 += 1\n",
        "            #     if len(review_list) < 64:\n",
        "            #         len_64 += 1\n",
        "            #     if len(review_list) < 32:\n",
        "            #         len_32 += 1\n",
        "            #     if len(review_list) < 16:\n",
        "            #         len_16 += 1\n",
        "            #     if len(review_list) < 8:\n",
        "            #         len_8 += 1\n",
        "            #     if len(review_list) < 512:\n",
        "            #         len_512 += 1\n",
        "            # print('len_8: ' + str(len_8))\n",
        "            # print('len_16: ' + str(len_16))\n",
        "            # print('len_32: ' + str(len_32))\n",
        "            # print('len_64: ' + str(len_64))\n",
        "            # print('len_128: ' + str(len_128))\n",
        "            # print('len_256: ' + str(len_256))\n",
        "            # print('len_512: ' + str(len_512))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
        "                inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            # 내가 추가한 부분\n",
        "            # logits = batch[0]\n",
        "            # tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n",
        "            #     args.model_name_or_path,\n",
        "            #     do_lower_case=args.do_lower_case\n",
        "            # )\n",
        "            #\n",
        "            # temp = logits.detach().cpu().numpy()\n",
        "            # for i in range(len(temp)):\n",
        "            #     # print(i)\n",
        "            #     review_list = list(temp[i])\n",
        "            #     while 0 in review_list:\n",
        "            #         review_list.remove(0)\n",
        "            #     del review_list[0]\n",
        "            #     del review_list[-1]\n",
        "            #     review_list = np.asarray(review_list)\n",
        "            #     print(tokenizer.decode(review_list), batch[3][i])\n",
        "            ##\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0 or (\n",
        "                    len(train_dataloader) <= args.gradient_accumulation_steps\n",
        "                    and (step + 1) == len(train_dataloader)\n",
        "            ):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                # 내가 수정한 부분\n",
        "                print(\"loss: \" + str(tr_loss / global_step), end=\"\\r\")\n",
        "                # sleep(0.1)\n",
        "                # pbar.update()\n",
        "                #\n",
        "\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    if args.evaluate_test_during_training:\n",
        "                        evaluate(args, model, full_text, test_dataset, \"test\", global_step)\n",
        "\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to {}\".format(output_dir))\n",
        "\n",
        "                    if args.save_optimizer:\n",
        "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                        logger.info(\"Saving optimizer and scheduler states to {}\".format(output_dir))\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                break\n",
        "\n",
        "        mb.write(\"Epoch {} done\".format(epoch + 1))\n",
        "\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            break\n",
        "\n",
        "    return global_step, tr_loss / global_step, full_text"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghFXmNuqq7q7"
      },
      "source": [
        "def evaluate(args, model, full_text, eval_dataset, mode, global_step=None):\n",
        "    results = {}\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    if global_step != None:\n",
        "        logger.info(\"***** Running evaluation on {} dataset ({} step) *****\".format(mode, global_step))\n",
        "    else:\n",
        "        logger.info(\"***** Running evaluation on {} dataset *****\".format(mode))\n",
        "    logger.info(\"  Num examples = {}\".format(len(eval_dataset)))\n",
        "    logger.info(\"  Eval Batch size = {}\".format(args.eval_batch_size))\n",
        "    eval_loss = 0.0\n",
        "    # nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "    out_input_ids = None\n",
        "\n",
        "    for batch in progress_bar(eval_dataloader):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"labels\": batch[3]\n",
        "            }\n",
        "\n",
        "            # 내가 쓴 곳\n",
        "            # tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n",
        "            #     args.model_name_or_path,\n",
        "            #     do_lower_case=args.do_lower_case\n",
        "            # )\n",
        "            # for i in range(len(inputs)):\n",
        "            #     print(tokenizer.decode(inputs['input_ids'][i]), inputs['labels'])\n",
        "\n",
        "            if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
        "                inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        # nb_eval_steps += 1\n",
        "        if preds is None:\n",
        "            out_input_ids = inputs['input_ids'].detach().cpu().numpy()\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "        else:\n",
        "            out_input_ids = np.append(out_input_ids, inputs['input_ids'].detach().cpu().numpy(), axis=0)\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        # 내가 수정한 부분\n",
        "\n",
        "    tokenizer = args.tokenizer.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case\n",
        "    )\n",
        "    # label_dict = {'칭찬': 0, '중립': 0.5, '불만': 1}\n",
        "    label_dict = {'None': 0, '상담원': 1, '상담시스템': 2, '고객서비스': 3, '혜택': 4, '할부금융상품': 5,\n",
        "                  '커뮤니티서비스': 6, '카드이용/결제': 7, '카드상품': 8, '청구입금': 9, '심사/한도': 10,\n",
        "                  '생활편의서비스': 11, '상담/채널': 12, '리스렌탈상품': 13, '라이프서비스': 14, '금융상품': 15,\n",
        "                  '고객정보관리': 16, '가맹점매출/승인': 17, '가맹점대금': 18, '가맹점계약': 19, '삼성카드': 20, '기타': 21}\n",
        "    label_dict = dict((v, k) for k, v in label_dict.items())\n",
        "    df_review = []\n",
        "    # temp_review = []\n",
        "    df_label = np.vectorize(label_dict.get)(out_label_ids)\n",
        "    df_prediction = np.vectorize(label_dict.get(np.argmax(preds)))\n",
        "    for i in range(len(out_input_ids)):\n",
        "        review_list = list(out_input_ids[i])\n",
        "\n",
        "\n",
        "        # temp_review.append(str(x) for x in out_input_ids[i])\n",
        "\n",
        "\n",
        "        while 0 in review_list:\n",
        "            review_list.remove(0)\n",
        "        del review_list[0]\n",
        "        del review_list[-1]\n",
        "        df_review.append(tokenizer.decode(review_list))\n",
        "        # print(review_list, label_dict[out_label_ids[i] - 1], label_dict[np.argmax(preds[i]) - 1])\n",
        "    df_data = {'Review': df_review, 'Label': df_label, 'Prediction': df_prediction}\n",
        "    df = pd.DataFrame(df_data)\n",
        "\n",
        "    df_train_data = {'Review': full_text['Review'], 'Label': full_text['Label']}\n",
        "    df_from_train = pd.DataFrame(df_train_data)\n",
        "    # Dodged Bar Chart (with same X coordinates side by side)\n",
        "\n",
        "    bar_width = 0.35\n",
        "    alpha = 0.5\n",
        "    label_lst = list(label_dict.values())\n",
        "    index = np.arange(len(label_lst))\n",
        "    # print(index)\n",
        "    count_list, cnt_pred, cnt_label = [0 for _ in range(len(label_dict))], \\\n",
        "                                    [0 for _ in range(len(label_dict))], [0 for _ in range(len(label_dict))]\n",
        "    count_labels = df_from_train.groupby('Label', as_index=False).Review.count()\n",
        "    acc_labels = df[df['Label'] == df['Prediction']].groupby('Label').Review.count()\n",
        "    # print('hey', df_from_train['Label'][5])\n",
        "    # print(count_labels)\n",
        "    viable_label = list(count_labels['Label'])\n",
        "    for i in range(len(viable_label)):\n",
        "        count_list[viable_label[i]] = count_labels['Review'][i]\n",
        "    # for i in range(len(df_from_train['Label'])):\n",
        "    #     print(count_list[i], count_labels[i])\n",
        "    #     count_list[int(df_from_train['Label'][i])] = count_labels[i]\n",
        "    # print('out', len(out_label_ids))\n",
        "    # print(len(list(out_label_ids)))\n",
        "    for validation in range(len(out_label_ids)):\n",
        "        if out_label_ids[validation] == np.argmax(preds, axis=1)[validation]:\n",
        "            cnt_pred[out_label_ids[validation]] += 100\n",
        "        cnt_label[out_label_ids[validation]] += 1\n",
        "    for i in range(len(cnt_label)):\n",
        "        if cnt_label[i] == 0:\n",
        "            cnt_label[i] = 1\n",
        "    acc_tot = np.divide(cnt_pred, cnt_label)\n",
        "    acc_tot[np.isnan(acc_tot)] = 0\n",
        "    # print(count_list)\n",
        "    # print(acc_tot)\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title('Bar Chart of Labels Count and Accuracy', fontsize=15)\n",
        "    clrs = ['r' if (x < 15) else 'b' for x in count_list]\n",
        "    p1 = plt.bar(index, count_list,\n",
        "                 bar_width,\n",
        "                 color=clrs,\n",
        "                 alpha=alpha,\n",
        "                 label='Count')\n",
        "    plt.ylabel('Count of Labels', fontsize=12)\n",
        "    plt.xticks([], [])\n",
        "    # plt.legend((p1[0],), ('Count',), fontsize=10)\n",
        "    plt.subplot(2, 1, 2)\n",
        "    p2 = plt.bar(index + bar_width, acc_tot,\n",
        "                 bar_width,\n",
        "                 color=clrs,\n",
        "                 alpha=alpha,\n",
        "                 label='Accuracy')\n",
        "    plt.gca().yaxis.set_major_formatter(mticker.PercentFormatter())\n",
        "    plt.ylabel('Accuracy by Labels', fontsize=12)\n",
        "    plt.xlabel('Label', fontsize=12)\n",
        "    plt.xticks(index, label_lst, fontsize=10, rotation=45)\n",
        "    # plt.legend((p2[0],), ('Accuracy',), fontsize=10)\n",
        "    plt.show()\n",
        "        # for i in range(len(out_label_ids)):\n",
        "        #     print(tokenizer.decode(out_ids[i]), out_label_ids[i], preds[i])\n",
        "        # print(type(out_label_ids), type(preds))\n",
        "        # print(out_label_ids, preds)\n",
        "        #\n",
        "\n",
        "    # eval_loss = eval_loss / nb_eval_steps\n",
        "    # if output_modes[args.task] == \"classification\":\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    # elif output_modes[args.task] == \"regression\":\n",
        "    #     preds = np.squeeze(preds)\n",
        "    result = compute_metrics(out_label_ids, preds)\n",
        "\n",
        "    # numpy_data = np.array(out_label_ids, preds)\n",
        "    # df = pd.DataFrame(data=numpy_data, index=[\"row1\", \"row2\"], columns=[\"column1\", \"column2\"])\n",
        "\n",
        "    results.update(result)\n",
        "    output_dir = os.path.join(args.output_dir, mode)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    output_eval_file = os.path.join(output_dir,\n",
        "                                    \"{}-{}.txt\".format(mode, global_step) if global_step else \"{}.txt\".format(mode))\n",
        "    with open(output_eval_file, \"w\") as f_w:\n",
        "        logger.info(\"***** Eval results on {} dataset *****\".format(mode))\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  {} = {}\".format(key, str(results[key])))\n",
        "            f_w.write(\"  {} = {}\\n\".format(key, str(results[key])))\n",
        "\n",
        "    return results\n",
        "\n",
        "def compute_metrics(labels, preds):\n",
        "    assert len(preds) == len(labels)\n",
        "    return (labels == preds).mean()"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxQ2yD-kN-H",
        "outputId": "abfb67aa-c74f-4948-fba0-58af2715cdee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551,
          "referenced_widgets": [
            "3e184ffbc3ed452e82c305237ba2fa66",
            "f9f61f6bb827492b809810baf00761c6",
            "865583cbea934829badf857d7b79cceb",
            "22bf8ed41a7047d2a5b317b575ed27f6",
            "e61916f6dec240b8aa7fb14c393a291e",
            "d480d2de88764092a2a6cec4afdf2362",
            "04362671776549569cf73a1381f9e08e",
            "05010de22972430886d1c495933b93ad"
          ]
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# GPU or CPU\n",
        "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "model.to(args.device)\n",
        "\n",
        "# Train\n",
        "global_step, tr_loss, full_text = train(args, model, train_dataset, test_dataset)\n",
        "logger.info(\" global_step = {}, average loss = {}\".format(global_step, tr_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/05/2021 09:57:55 - INFO - __main__ -   ***** Running training *****\n",
            "08/05/2021 09:57:55 - INFO - __main__ -     Num examples = 7200\n",
            "08/05/2021 09:57:55 - INFO - __main__ -     Num Epochs = 1\n",
            "08/05/2021 09:57:55 - INFO - __main__ -     Total train batch size = 32\n",
            "08/05/2021 09:57:55 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "08/05/2021 09:57:55 - INFO - __main__ -     Total optimization steps = 225\n",
            "08/05/2021 09:57:55 - INFO - __main__ -     Logging steps = 1\n",
            "08/05/2021 09:57:55 - INFO - __main__ -     Save steps = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                 Review  Label\n",
            "0     다른 카드사보다 명세서가 많이 늦습니다  11일 결제일인데 적어도 5일 이내로 도착...      9\n",
            "1      재난지원금 쓰는데 왜 한도가 뭐가 필요하죠   한도를 올려주던가요   재난지원금음...     11\n",
            "2     홈페이지 사용에서 안내하신 분의 도움으로 해결은 하였지만  쉽게 알아볼 수 있도록 ...     12\n",
            "3                                앱 사용을 직접 못하다 보니 Ars로 함     12\n",
            "4                말은  천천히 상대방이 알아듣고 있나  확인 필요합니다   감사합니다      1\n",
            "...                                                 ...    ...\n",
            "8995                                             수고하세요      21\n",
            "8996                                            너무 만족해요     21\n",
            "8997                               친절하고 신속한 답변 감사합니다         1\n",
            "8998                                     친절한 응대에 감사드립니다      1\n",
            "8999  연회비 면제 카드로  알고  있었는데 엊그제 카드대금 관련  문자에 연회비 청구 포...      9\n",
            "\n",
            "[9000 rows x 2 columns]\n",
            "max steps: 225 length of train data: 225 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e184ffbc3ed452e82c305237ba2fa66",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/225 00:00<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "08/05/2021 09:58:32 - INFO - __main__ -   ***** Running evaluation on test dataset (1 step) *****\n",
            "08/05/2021 09:58:32 - INFO - __main__ -     Num examples = 1800\n",
            "08/05/2021 09:58:32 - INFO - __main__ -     Eval Batch size = 32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 2.5702767372131348\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='21' class='' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      36.84% [21/57 04:01<06:53]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FrIV92vp7fw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ9L624hmncm"
      },
      "source": [
        "results = {}\n",
        "checkpoints = list(\n",
        "            os.path.dirname(c) for c in\n",
        "            sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "        )\n",
        "        if not args.eval_all_checkpoints:\n",
        "            checkpoints = checkpoints[-1:]\n",
        "        else:\n",
        "            logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1]\n",
        "            model = args.model.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, full_text, test_dataset, mode=\"test\", global_step=global_step)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as f_w:\n",
        "            for key in sorted(results.keys()):\n",
        "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOFBaYi8S7h6"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_end_time = printt(\"Model building: Start\")\n",
        "print(_model_build_end_time - _model_build_start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZX6jC22S99-"
      },
      "source": [
        "#모델 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPEMmgXfTKf9"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_start_time = printt(\"TEST: Start\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLDvBcGMSdwg"
      },
      "source": [
        "#TODO: 해당 블럭에 테스트 수행을 위한 코드를 넣으세요. (시간측정 구간)\n",
        "#분석 파일은 tsv 파일로 제공되며, 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 =공백)으로 제공됩니다.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLsLL-vSq1D"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_end_time = printt(\"Model building: Start\")\n",
        "print(_test_end_time - _test_start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_NifWnpKw-"
      },
      "source": [
        "# 결과출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UctiMXI8UQ0Q"
      },
      "source": [
        "#TODO:해당 블럭에 테스트 결과를 파일로 저장하는 코드를 넣으세요. (시간측정 제외)\n",
        "#저장 파일은tsv 파일로 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 = 테스트 결과 도출된 양식)으로 저장\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmpdIIPmkMez"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}