{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "222번.카이의김씨들(김기영,김주희,김재훈).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGxU_ZS8DEE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEZSFwyP5fQ"
      },
      "source": [
        "#공용함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUkWw1txP4XA"
      },
      "source": [
        "# 수정금지: 타임스탬프용 함수\n",
        "from datetime import datetime\n",
        "def printt(*args,**kwargs):\n",
        "  now = datetime.now()\n",
        "  now_str = \"{:02}:{:02}:{:02}\".format(now.hour,now.minute,now.second)\n",
        "  print(now_str, *args,**kwargs)\n",
        "  return int(now.hour)*60*60+int(now.minute)*60+int(now.second)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-h1MVpOSRVl"
      },
      "source": [
        "#연관 패키지 설치 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOHw8KJbRC4m"
      },
      "source": [
        "#TODO: 해당 블럭에 패키지 설치하세요.\n",
        "!pip install attrdict\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "!pip install fastprogress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIMuPDLIo0rb"
      },
      "source": [
        "# 파일로딩 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ4zJTn7o0LO"
      },
      "source": [
        "#TODO: 해당 블럭에 필요 파일 로딩 코드 넣으시오.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTVIEOA0Swkd"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOoJjT8Sx3R"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_start_time = printt(\"Model building: Start\")\n",
        "_model_build_start_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxnQUVccSa-V"
      },
      "source": [
        "#TODO: 블럭에 모델 학습 - 빌딩 코드를 넣으세요. (시간측정 구간)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIUoYKVugaUk"
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVLP6QJlgfuP"
      },
      "source": [
        "# 실행 전 런타임 다시시작\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "plt.plot([1, 2, 3, 4])\n",
        "plt.ylabel('한국어 테스트')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qi3tiTrV5Gy"
      },
      "source": [
        "task = 'classification'\n",
        "# task = 'sentiment'\n",
        "train_file = 'train_cls.txt'\n",
        "test_file = 'evaluation_cls.txt'\n",
        "full_text = 'full_text.txt'\n",
        "\n",
        "\n",
        "import os\n",
        "!git clone https://github.com/Jaehun-Kim22/SCIC_Project.git\n",
        "\n",
        "# train_file = train_s_cls.txt\n",
        "# test_file = test_s_cls.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJI_EzPkISd9"
      },
      "source": [
        "# initialize logging\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format= \"%(asctime)s - %(message)s\",\n",
        "    # format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaOFiXQm_xSk"
      },
      "source": [
        "# make Input Example\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, review_token, label):\n",
        "        self.review_token = review_token\n",
        "        self.label = label\n",
        "\n",
        "    # def __repr__(self):\n",
        "    #     return str(self.to_json_string())\n",
        "\n",
        "    # 2번째\n",
        "    # def to_dict(self):\n",
        "    #     \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "    #     output = copy.deepcopy(self.__dict__)\n",
        "    #     return output\n",
        "\n",
        "    # 1번째\n",
        "    # def to_json_string(self):\n",
        "    #     \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "    #     return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "        \n",
        "# make Input Feature\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label = label\n",
        "\n",
        "    # def __repr__(self):\n",
        "    #     return str(self.to_json_string())\n",
        "\n",
        "    # def to_dict(self):\n",
        "    #     \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "    #     output = copy.deepcopy(self.__dict__)\n",
        "    #     return output\n",
        "\n",
        "    # def to_json_string(self):\n",
        "    #     \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "    #     return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "# make processor\n",
        "class Processor(object):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "    def get_labels(self):\n",
        "        if self.args.task == 'classification':\n",
        "        # return ['None', '상담원', '상담시스템', '고객서비스', '혜택', '할부금융상품', '커뮤니티서비스',\n",
        "        #         '카드이용/결제', '카드상품', '청구입금', '심사/한도', '생활편의서비스', '상담/채널', '리스렌탈상품',\n",
        "        #         '라이프서비스', '금융상품', '고객정보관리', '가맹점매출/승인', '가맹점대금', '가맹점계약', '삼성카드', '기타']\n",
        "          return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
        "                  '13', '14', '15', '16']\n",
        "        else:\n",
        "          return [None]\n",
        "          # 11번째\n",
        "          # return ['0', '1']\n",
        "\n",
        "    def _read_file(cls, input_file):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = []\n",
        "            for line in f:\n",
        "                lines.append(line.strip())\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines[0:]):\n",
        "            line = line.split(\"\\t\")\n",
        "            review_token = line[0]\n",
        "            label = line[1]\n",
        "            if i % 10000 == 0:\n",
        "                logger.info(line)\n",
        "            examples.append(InputExample(review_token=review_token, label=label))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, test\n",
        "        \"\"\"\n",
        "        file_to_read = None\n",
        "        if mode == \"train\":\n",
        "            file_to_read = self.args.train_file\n",
        "        elif mode == \"test\":\n",
        "            file_to_read = self.args.test_file\n",
        "\n",
        "        logger.info(\"LOOKING AT {}\".format(os.path.join(self.args.data_dir, file_to_read)))\n",
        "        return self._create_examples(\n",
        "            self._read_file(os.path.join(self.args.data_dir, file_to_read)), mode\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN4qSwp3_2P8"
      },
      "source": [
        "# set config args for classification\n",
        "from transformers import (\n",
        "    ElectraConfig,\n",
        "    ElectraTokenizer,\n",
        "    ElectraForSequenceClassification,\n",
        "    XLMRobertaForSequenceClassification,\n",
        "    XLMRobertaTokenizer,\n",
        "    XLMRobertaConfig\n",
        ")\n",
        "from attrdict import AttrDict\n",
        "args = AttrDict(\n",
        "    {\n",
        "                 'data_dir': 'SCIC_Project/assets/data/{}'.format(task),\n",
        "                 'train_file': train_file,\n",
        "                 'test_file': test_file,\n",
        "                 'task': task,\n",
        "                 'config': ElectraConfig,\n",
        "                 'tokenizer': ElectraTokenizer,\n",
        "                 'model': ElectraForSequenceClassification,\n",
        "                #  'config': XLMRobertaConfig,\n",
        "                #  'tokenizer': XLMRobertaTokenizer,\n",
        "                #  'model': XLMRobertaForSequenceClassification,\n",
        "                 'evaluate_test_during_training': True, \n",
        "                 'eval_all_checkpoints': True, \n",
        "                 'save_optimizer': False, \n",
        "                 'do_lower_case': False, \n",
        "                 'do_train': True, \n",
        "                 'do_eval': True, \n",
        "                 'max_seq_len': 128, \n",
        "                 'num_train_epochs': 10, \n",
        "                 'weight_decay': 0.0, \n",
        "                 'gradient_accumulation_steps': 1, \n",
        "                 'adam_epsilon': 1e-08, \n",
        "                 'warmup_proportion': 0, \n",
        "                 'max_steps': -1, \n",
        "                 'max_grad_norm': 1.0, \n",
        "                 'no_cuda': False, \n",
        "                 'model_type': 'koelectra-base-v3', \n",
        "                 'model_name_or_path': 'monologg/koelectra-base-v3-discriminator', \n",
        "                 'output_dir': '{}_checkpoints'.format(task), \n",
        "                 'seed': 42, \n",
        "                 'train_batch_size': 32, \n",
        "                 'eval_batch_size': 128, \n",
        "                 'logging_steps': 250, \n",
        "                 'save_steps': 250, \n",
        "                 'learning_rate': 5e-06\n",
        "     }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bgL2T7UEldh"
      },
      "source": [
        "print(train_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUZThiu_893"
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# set seed\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "#TODO: log versioning is not yet done\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6-cH8FJUtst"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkNkINJAUpsA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfIJc-O3AAuH"
      },
      "source": [
        "# %cd /content/drive/MyDrive/Colab Notebooks\n",
        "import os\n",
        "from torch.utils.data import TensorDataset\n",
        "logger = logging.getLogger(__name__)\n",
        "print(__name__)\n",
        "def load_and_cache_examples(args, tokenizer, mode):\n",
        "    processor = Processor(args)\n",
        "    output_mode = args.task\n",
        "    # Load data features from cache or dataset file\n",
        "\n",
        "    #4 번째\n",
        "    # cached_features_file = os.path.join(\n",
        "    #     args.data_dir,\n",
        "    #     \"cached_{}_{}_{}_{}\".format(\n",
        "    #         str(args.task), list(filter(None, args.model_name_or_path.split(\"/\"))).pop(), str(args.max_seq_len), mode\n",
        "    #     ),\n",
        "    # )\n",
        "\n",
        "    # 3 번째\n",
        "    # if os.path.exists(cached_features_file):\n",
        "    #     logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "    #     features = torch.load(cached_features_file)\n",
        "    # else:\n",
        "    logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "    if mode == \"train\":\n",
        "        examples = processor.get_examples(\"train\")\n",
        "    elif mode == \"test\":\n",
        "        examples = processor.get_examples(\"test\")\n",
        "    else:\n",
        "        raise ValueError(\"For mode, choose between train and test\")\n",
        "    features = seq_cls_convert_examples_to_features(\n",
        "        args, examples, tokenizer, max_length=args.max_seq_len\n",
        "    )\n",
        "    # 4 번째\n",
        "    # logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "\n",
        "    # 3 번째\n",
        "    # torch.save(features, cached_features_file)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "\n",
        "\n",
        "    # 5 번째\n",
        "    ## if output_mode == \"classification\" or 'sentiment':\n",
        "    # all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "\n",
        "    # 6 번째\n",
        "    # all_labels = None\n",
        "    if args.task == \"classification\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    else:\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "    return dataset\n",
        "def seq_cls_convert_examples_to_features(args, examples, tokenizer, max_length):\n",
        "    processor = Processor(args)\n",
        "    label_list = processor.get_labels()\n",
        "    output_mode = args.task\n",
        "    logger.info(\"Using label list {}\".format(label_list))\n",
        "    logger.info(\"Using output mode {}\".format(args.task))\n",
        "\n",
        "    # labels = [label_from_example(example) for example in examples]\n",
        "\n",
        "    # 8번째\n",
        "    # labels = [int(example.label) for example in examples]\n",
        "\n",
        "    if args.task == 'classification':\n",
        "      labels = [int(example.label) for example in examples]\n",
        "    else:\n",
        "      labels = [float(example.label) for example in examples]\n",
        "\n",
        "    batch_encoding = tokenizer.batch_encode_plus(\n",
        "        [(example.review_token) for example in examples],\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    for i in range(len(examples)):\n",
        "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "\n",
        "        # 6번째\n",
        "        # if \"token_type_ids\" not in inputs:\n",
        "        inputs[\"token_type_ids\"] = [0] * len(inputs[\"input_ids\"])  # For xlm-roberta\n",
        "\n",
        "        feature = InputFeatures(**inputs, label=labels[i])\n",
        "        features.append(feature)\n",
        "\n",
        "    for i, example in enumerate(examples[:1]):\n",
        "        logger.info(\"*** Example ***\")\n",
        "        # logger.info(\"guid: {}\".format(example.guid))\n",
        "        logger.info(\"input_ids: {}\".format(\" \".join([str(x) for x in features[i].input_ids])))\n",
        "        logger.info(\"attention_mask: {}\".format(\" \".join([str(x) for x in features[i].attention_mask])))\n",
        "        # logger.info(\"token_type_ids: {}\".format(\" \".join([str(x) for x in features[i].token_type_ids])))\n",
        "        logger.info(\"label: {}\".format(features[i].label))\n",
        "\n",
        "    return features\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZXbz67inCgG"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def train(args,\n",
        "          model,\n",
        "          train_dataset,\n",
        "          test_dataset=None):\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "    full_text = pd.read_csv(os.path.join(args.data_dir, 'full_text.txt'), sep='\\t')\n",
        "    # full_text = pd.read_csv('train_n_full.txt', sep='\\t')\n",
        "    print(full_text)\n",
        "\n",
        "    # 7번째\n",
        "    # if args.max_steps > 0:\n",
        "    #     t_total = args.max_steps\n",
        "    #     print()\n",
        "    #     print(t_total, len(train_dataloader), args.gradient_accumulation_steps)\n",
        "    #     print()\n",
        "    #     args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    # else:\n",
        "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "    print('max steps: ' + str(t_total), 'length of train data: ' + str(len(train_dataloader)),\n",
        "          args.gradient_accumulation_steps)\n",
        "    \n",
        "    \n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(t_total * args.warmup_proportion),\n",
        "                                                num_training_steps=t_total)\n",
        "\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Total train batch size = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "    logger.info(\"  Logging steps = %d\", args.logging_steps)\n",
        "    logger.info(\"  Save steps = %d\", args.save_steps)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss = 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    for epoch in tqdm(mb):\n",
        "        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
        "\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"labels\": batch[3]\n",
        "            }\n",
        "\n",
        "            if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
        "                inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0 or (\n",
        "                    len(train_dataloader) <= args.gradient_accumulation_steps\n",
        "                    and (step + 1) == len(train_dataloader)\n",
        "            ):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "\n",
        "                # print(\"loss: \" + str(tr_loss / global_step))\n",
        "                writer.add_scalar(tag='tr_loss',\n",
        "                                  scalar_value=tr_loss / global_step,\n",
        "                                  global_step=global_step)\n",
        "\n",
        "\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    if args.evaluate_test_during_training:\n",
        "                        evaluate(args, model, full_text, test_dataset, \"test\", global_step)\n",
        "\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to {}\".format(output_dir))\n",
        "\n",
        "                    if args.save_optimizer:\n",
        "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                        logger.info(\"Saving optimizer and scheduler states to {}\".format(output_dir))\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                break\n",
        "\n",
        "        mb.write(\"Epoch {} done\".format(epoch + 1))\n",
        "\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            break\n",
        "\n",
        "    return global_step, tr_loss / global_step, full_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghFXmNuqq7q7"
      },
      "source": [
        "def evaluate(args, model, full_text, eval_dataset, mode, global_step=None):\n",
        "    results = {}\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    if global_step != None:\n",
        "        logger.info(\"***** Running evaluation on {} dataset ({} step) *****\".format(mode, global_step))\n",
        "    else:\n",
        "        logger.info(\"***** Running evaluation on {} dataset *****\".format(mode))\n",
        "    logger.info(\"  Num examples = {}\".format(len(eval_dataset)))\n",
        "    logger.info(\"  Eval Batch size = {}\".format(args.eval_batch_size))\n",
        "    eval_loss = 0.0\n",
        "    # nb_eval_steps = 0\n",
        "    preds = None\n",
        "    preds_save = None\n",
        "    out_label_ids = None\n",
        "    out_input_ids = None\n",
        "\n",
        "    for batch in progress_bar(eval_dataloader):\n",
        "        preds_temp = None\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"labels\": batch[3]\n",
        "            }\n",
        "\n",
        "            if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
        "                inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        # nb_eval_steps += 1\n",
        "        if preds is None:\n",
        "            out_input_ids = inputs['input_ids'].detach().cpu().numpy()\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            preds_save = preds.copy()\n",
        "            # 12번째\n",
        "            # if args.task == 'sentiment':\n",
        "            #   for i in range(len(preds)):\n",
        "            #     if preds[i][0] < 0.5:\n",
        "            #       preds[i][0] = 0\n",
        "            #     else:\n",
        "            #       preds[i][0] = 1\n",
        "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "        else:\n",
        "            out_input_ids = np.append(out_input_ids, inputs['input_ids'].detach().cpu().numpy(), axis=0)\n",
        "            preds_temp = logits.detach().cpu().numpy()\n",
        "            # 12번째\n",
        "            # if args.task == 'sentiment':\n",
        "            #   for i in range(len(preds_temp)):\n",
        "            #     if preds_temp[i][0] < 0.5:\n",
        "            #       print('yes')\n",
        "            #       preds_temp[i][0] = 0\n",
        "            #     else:\n",
        "            #       preds_temp[i][0] = 1\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            preds_save = np.append(preds_save, logits.detach().cpu().numpy(), axis=0)\n",
        "            # preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        # 내가 수정한 부분\n",
        "        \n",
        "        if args.task == 'sentiment':\n",
        "          for i in range(len(preds)):\n",
        "            if preds[i][0] < 0.5:\n",
        "              preds[i][0] = 0\n",
        "            else:\n",
        "              preds[i][0] = 1\n",
        "    tokenizer = args.tokenizer.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case\n",
        "    )\n",
        "    # label_dict = {'칭찬': 0, '중립': 0.5, '불만': 1}\n",
        "    if args.task == 'classification':\n",
        "      label_dict = {'중립': 0, '상담원': 1, '상담시스템': 2, '혜택': 3, '할부금융상품': 4,\n",
        "                  '카드상품': 5, '청구입금': 6, '심사/한도': 7, '생활편의서비스': 8,\n",
        "                  '상담/채널': 9, '리스렌탈상품': 10, '라이프서비스': 11, '금융상품': 12,\n",
        "                  '고객정보관리': 13, '가맹점매출/승인': 14, '삼성카드': 15, '기타': 16}\n",
        "    else:\n",
        "      label_dict = {'칭찬': 0, '불만': 1}\n",
        "    label_dict = dict((v, k) for k, v in label_dict.items())\n",
        "    df_review = []\n",
        "    # temp_review = []\n",
        "    df_label = np.vectorize(label_dict.get)(out_label_ids)\n",
        "    df_prediction = np.vectorize(label_dict.get(np.argmax(preds)))\n",
        "    for i in range(len(out_input_ids)):\n",
        "        review_list = list(out_input_ids[i])\n",
        "\n",
        "\n",
        "        # temp_review.append(str(x) for x in out_input_ids[i])\n",
        "\n",
        "\n",
        "        while 0 in review_list:\n",
        "            review_list.remove(0)\n",
        "        del review_list[0]\n",
        "        del review_list[-1]\n",
        "        df_review.append(tokenizer.decode(review_list))\n",
        "        # print(review_list, label_dict[out_label_ids[i] - 1], label_dict[np.argmax(preds[i]) - 1])\n",
        "    df_data = {'Review': df_review, 'Label': df_label, 'Prediction': df_prediction}\n",
        "    df = pd.DataFrame(df_data)\n",
        "\n",
        "    df_train_data = {'Review': full_text['Review'], 'Label': full_text['Label']}\n",
        "    df_from_train = pd.DataFrame(df_train_data)\n",
        "    # Dodged Bar Chart (with same X coordinates side by side)\n",
        "\n",
        "    bar_width = 0.35\n",
        "    alpha = 0.5\n",
        "    label_lst = list(label_dict.values())\n",
        "    index = np.arange(len(label_lst))\n",
        "    count_list, cnt_pred, cnt_label = [0 for _ in range(len(label_dict))], \\\n",
        "                                    [0 for _ in range(len(label_dict))], [0 for _ in range(len(label_dict))]\n",
        "    count_labels = df_from_train.groupby('Label', as_index=False).Review.count()\n",
        "    acc_labels = df[df['Label'] == df['Prediction']].groupby('Label').Review.count()\n",
        "    viable_label = list(count_labels['Label'])\n",
        "    for i in range(len(viable_label)):\n",
        "        count_list[viable_label[i]] = count_labels['Review'][i]\n",
        "    for validation in range(len(out_label_ids)):\n",
        "        if args.task == 'sentiment':\n",
        "          if int(out_label_ids[validation]) == int(preds[validation][0]):\n",
        "              cnt_pred[int(out_label_ids[validation])] += 100\n",
        "          cnt_label[int(out_label_ids[validation])] += 1\n",
        "        else:\n",
        "          if out_label_ids[validation] == np.argmax(preds, axis=1)[validation]:\n",
        "              cnt_pred[int(out_label_ids[validation])] += 100\n",
        "          cnt_label[int(out_label_ids[validation])] += 1\n",
        "    for i in range(len(cnt_label)):\n",
        "        if cnt_label[i] == 0:\n",
        "            cnt_label[i] = 1\n",
        "    acc_tot = np.divide(cnt_pred, cnt_label)\n",
        "    acc_tot[np.isnan(acc_tot)] = 0\n",
        "    print(count_list)\n",
        "    print(acc_tot)\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title('Bar Chart of Labels Count and Accuracy', fontsize=15)\n",
        "    clrs = ['r' if (x < 15) else 'b' for x in count_list]\n",
        "    p1 = plt.bar(index, count_list,\n",
        "                 bar_width,\n",
        "                 color=clrs,\n",
        "                 alpha=alpha,\n",
        "                 label='Count')\n",
        "    plt.ylabel('Count of Labels', fontsize=12)\n",
        "    plt.xticks([], [])\n",
        "    # plt.legend((p1[0],), ('Count',), fontsize=10)\n",
        "    plt.subplot(2, 1, 2)\n",
        "    p2 = plt.bar(index + bar_width, acc_tot,\n",
        "                 bar_width,\n",
        "                 color=clrs,\n",
        "                 alpha=alpha,\n",
        "                 label='Accuracy')\n",
        "    plt.gca().yaxis.set_major_formatter(mticker.PercentFormatter())\n",
        "    plt.ylabel('Accuracy by Labels', fontsize=12)\n",
        "    plt.xlabel('Label', fontsize=12)\n",
        "    plt.xticks(index, label_lst, fontsize=10, rotation=45)\n",
        "    # plt.legend((p2[0],), ('Accuracy',), fontsize=10)\n",
        "    # plt.show()\n",
        "        # for i in range(len(out_label_ids)):\n",
        "        #     print(tokenizer.decode(out_ids[i]), out_label_ids[i], preds[i])\n",
        "        # print(type(out_label_ids), type(preds))\n",
        "        # print(out_label_ids, preds)\n",
        "        #\n",
        "\n",
        "    # log matplotlib to tensorboard\n",
        "    writer.add_figure('bar chart of labels count and accuracy', fig)\n",
        "\n",
        "    # eval_loss = eval_loss / nb_eval_steps\n",
        "    # if output_modes[args.task] == \"classification\":\n",
        "\n",
        "\n",
        "    preds_original = preds.copy()\n",
        "\n",
        "    # 9번째\n",
        "    # preds = np.argmax(preds, axis=1)\n",
        "    if args.task == 'classification':\n",
        "      preds = np.argmax(preds, axis=1)\n",
        "    else:\n",
        "      preds = np.squeeze(preds)\n",
        "\n",
        "\n",
        "    # elif output_modes[args.task] == \"regression\":\n",
        "    #     preds = np.squeeze(preds)\n",
        "    result = compute_metrics(out_label_ids, preds)\n",
        "\n",
        "    # 틀린 항목 정리\n",
        "    check_right = out_label_ids == preds\n",
        "    df = pd.DataFrame(columns=['Review', 'Label', 'Prediction', 'Softmax'])\n",
        "    for check_result in range(len(check_right)):\n",
        "        if not check_right[check_result]:\n",
        "            review_list = list(out_input_ids[check_result])\n",
        "            while 0 in review_list:\n",
        "                review_list.remove(0)\n",
        "            del review_list[0]\n",
        "            del review_list[-1]\n",
        "            df = df.append({'Review': tokenizer.decode(review_list), \n",
        "                           'Label': label_dict[out_label_ids[check_result]], \n",
        "                           'Prediction': label_dict[preds[check_result]],\n",
        "                           'Softmax': preds_save[check_result]},\n",
        "                           ignore_index=True\n",
        "                           )\n",
        "    \n",
        "    print(df.head())\n",
        "    df.to_csv(r'analysis_ckpt_{}_{}.csv'.format(args.task, global_step), header=None, index=None, sep='\\t', mode='a')\n",
        "            # print(tokenizer.decode(review_list), 'Label: ' + str(label_dict[out_label_ids[check_result]]) + ' Prediction: ' + str(\n",
        "            #     label_dict[preds[check_result]]))\n",
        "\n",
        "\n",
        "    # numpy_data = np.array(out_label_ids, preds)\n",
        "    # df = pd.DataFrame(data=numpy_data, index=[\"row1\", \"row2\"], columns=[\"column1\", \"column2\"])\n",
        "    print(result)\n",
        "    results.update(result)\n",
        "    output_dir = os.path.join(args.output_dir, mode)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    output_eval_file = os.path.join(output_dir,\n",
        "                                    \"{}-{}.txt\".format(mode, global_step) if global_step else \"{}.txt\".format(mode))\n",
        "    with open(output_eval_file, \"w\") as f_w:\n",
        "        logger.info(\"***** Eval results on {} dataset *****\".format(mode))\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  {} = {}\".format(key, str(results[key])))\n",
        "            f_w.write(\"  {} = {}\\n\".format(key, str(results[key])))\n",
        "\n",
        "    return results\n",
        "\n",
        "def compute_metrics(labels, preds):\n",
        "    assert len(preds) == len(labels)\n",
        "\n",
        "    # TODO: need to separate tr/val acc and loss + log all info in tensorboard\n",
        "    result = {\"acc\": (labels == preds).mean()}\n",
        "    writer.add_scalar(tag='val_acc',\n",
        "                      scalar_value=result['acc'])\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyPH-5ErasPI"
      },
      "source": [
        "if task == 'sentiment':\n",
        "  args.logging_steps, args.save_steps = 63, 63\n",
        "  args.max_seq_len = 64\n",
        "  args.train_batch_size = 128\n",
        "\n",
        "processor = Processor(args)\n",
        "labels = processor.get_labels()\n",
        "\n",
        "config = args.config.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            num_labels=len(labels)\n",
        "        )\n",
        "tokenizer = args.tokenizer.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    do_lower_case=args.do_lower_case\n",
        ")\n",
        "model = args.model.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "%cd /content/\n",
        "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
        "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygbv1CT5vKV3"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VARfUcFjGPam"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLsq-QkPGKhs"
      },
      "source": [
        "%tensorboard --logdir='runs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxQ2yD-kN-H"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# GPU or CPU\n",
        "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "model.to(args.device)\n",
        "\n",
        "# Train\n",
        "global_step, tr_loss, full_text = train(args, model, train_dataset, test_dataset)\n",
        "logger.info(\" global_step = {}, average loss = {}\".format(global_step, tr_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FrIV92vp7fw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ9L624hmncm"
      },
      "source": [
        "results = {}\n",
        "checkpoints = list(\n",
        "            os.path.dirname(c) for c in\n",
        "            sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "        )\n",
        "if not args.eval_all_checkpoints:\n",
        "    checkpoints = checkpoints[-1:]\n",
        "else:\n",
        "    logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "    logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "for checkpoint in checkpoints:\n",
        "    global_step = checkpoint.split(\"-\")[-1]\n",
        "    model = args.model.from_pretrained(checkpoint)\n",
        "    model.to(args.device)\n",
        "    result = evaluate(args, model, full_text, test_dataset, mode=\"test\", global_step=global_step)\n",
        "    result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "    results.update(result)\n",
        "\n",
        "output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "with open(output_eval_file, \"w\") as f_w:\n",
        "    for key in sorted(results.keys()):\n",
        "        f_w.write(\"{} = {}\\n\".format(key, str(results[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOFBaYi8S7h6"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_end_time = printt(\"Model building: Start\")\n",
        "print(_model_build_end_time - _model_build_start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZX6jC22S99-"
      },
      "source": [
        "#모델 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPEMmgXfTKf9"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_start_time = printt(\"TEST: Start\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLDvBcGMSdwg"
      },
      "source": [
        "#TODO: 해당 블럭에 테스트 수행을 위한 코드를 넣으세요. (시간측정 구간)\n",
        "#분석 파일은 tsv 파일로 제공되며, 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 =공백)으로 제공됩니다.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLsLL-vSq1D"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_end_time = printt(\"Model building: Start\")\n",
        "print(_test_end_time - _test_start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_NifWnpKw-"
      },
      "source": [
        "# 결과출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UctiMXI8UQ0Q"
      },
      "source": [
        "#TODO:해당 블럭에 테스트 결과를 파일로 저장하는 코드를 넣으세요. (시간측정 제외)\n",
        "#저장 파일은tsv 파일로 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 = 테스트 결과 도출된 양식)으로 저장\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmpdIIPmkMez"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR-CGR8LIVQW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uYU_MBjIYQr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}