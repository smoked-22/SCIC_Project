{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "222번.카이의김씨들(김기영,김주희,김재훈).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGxU_ZS8DEE"
      },
      "source": [
        "\"\"\"commented out for test execution\"\"\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEZSFwyP5fQ"
      },
      "source": [
        "#공용함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUkWw1txP4XA"
      },
      "source": [
        "# 수정금지: 타임스탬프용 함수\n",
        "from datetime import datetime\n",
        "def printt(*args,**kwargs):\n",
        "  now = datetime.now()\n",
        "  now_str = \"{:02}:{:02}:{:02}\".format(now.hour,now.minute,now.second)\n",
        "  print(now_str, *args,**kwargs)\n",
        "  return int(now.hour)*60*60+int(now.minute)*60+int(now.second)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-h1MVpOSRVl"
      },
      "source": [
        "#연관 패키지 설치 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOHw8KJbRC4m"
      },
      "source": [
        "#TODO: 해당 블럭에 패키지 설치하세요.\n",
        "!pip install attrdict\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "!pip install fastprogress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIMuPDLIo0rb"
      },
      "source": [
        "# 파일로딩 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ4zJTn7o0LO"
      },
      "source": [
        "#TODO: 해당 블럭에 필요 파일 로딩 코드 넣으시오.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTVIEOA0Swkd"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOoJjT8Sx3R"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_start_time = printt(\"Model building: Start\")\n",
        "_model_build_start_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxnQUVccSa-V"
      },
      "source": [
        "#TODO: 블럭에 모델 학습 - 빌딩 코드를 넣으세요. (시간측정 구간)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIUoYKVugaUk"
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVLP6QJlgfuP"
      },
      "source": [
        "# 실행 전 런타임 다시시작\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "plt.plot([1, 2, 3, 4])\n",
        "plt.ylabel('한국어 테스트')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qi3tiTrV5Gy"
      },
      "source": [
        "import os\n",
        "!git clone https://github.com/Jaehun-Kim22/SCIC_Project.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN4qSwp3_2P8"
      },
      "source": [
        "# set config args for classification\n",
        "from transformers import (\n",
        "    ElectraConfig,\n",
        "    ElectraTokenizer,\n",
        "    ElectraForSequenceClassification,\n",
        "    XLMRobertaForSequenceClassification,\n",
        "    XLMRobertaTokenizer,\n",
        "    XLMRobertaConfig\n",
        ")\n",
        "from attrdict import AttrDict\n",
        "CLASSIFICATION_LIST = (\n",
        "    ['None', '상담원', '상담시스템', '고객서비스', '혜택', '할부금융상품', '커뮤니티서비스',\n",
        "     '카드이용/결제', '카드상품', '청구입금', '심사/한도', '생활편의서비스', '상담/채널',\n",
        "     '리스렌탈상품','라이프서비스', '금융상품', '고객정보관리', '가맹점매출/승인', '가맹점대금',\n",
        "     '가맹점계약', '삼성카드', '기타']\n",
        ")\n",
        "CLASSIFICATION_DICT = {}\n",
        "for idx, item in enumerate(CLASSIFICATION_LIST):\n",
        "    CLASSIFICATION_DICT[str(idx)] = item\n",
        "\n",
        "args = AttrDict(\n",
        "    {\n",
        "                 'data_dir': 'SCIC_Project/assets/data',\n",
        "                 'train_file': 'train_cls.txt',\n",
        "                 'test_file': 'evaluation_cls.txt',\n",
        "                 'task': 'sentiment',\n",
        "                 'config': ElectraConfig,\n",
        "                 'tokenizer': ElectraTokenizer,\n",
        "                 'model': ElectraForSequenceClassification,\n",
        "                #  'config': XLMRobertaConfig,\n",
        "                #  'tokenizer': XLMRobertaTokenizer,\n",
        "                #  'model': XLMRobertaForSequenceClassification,\n",
        "                 'evaluate_test_during_training': True, \n",
        "                 'eval_all_checkpoints': True, \n",
        "                 'save_optimizer': False, \n",
        "                 'do_lower_case': False, \n",
        "                 'do_train': True, \n",
        "                 'do_eval': True, \n",
        "                 'max_seq_len': 128, \n",
        "                 'num_train_epochs': 10, \n",
        "                 'weight_decay': 0.0, \n",
        "                 'gradient_accumulation_steps': 1, \n",
        "                 'adam_epsilon': 1e-08, \n",
        "                 'warmup_proportion': 0, \n",
        "                 'max_steps': -1, \n",
        "                 'max_grad_norm': 1.0, \n",
        "                 'no_cuda': False, \n",
        "                 'model_type': 'koelectra-base-v3', \n",
        "                 'model_name_or_path': 'monologg/koelectra-base-v3-discriminator', \n",
        "                 'output_dir': '_checkpoints', \n",
        "                 'seed': 42, \n",
        "                 'train_batch_size': 32, \n",
        "                 'eval_batch_size': 32, \n",
        "                 'logging_steps': 250, \n",
        "                 'save_steps': 250, \n",
        "                 'learning_rate': 5e-05\n",
        "     }\n",
        ")\n",
        "args.data_dir = os.path.join(args.data_dir, args.task)\n",
        "args.output_dir = args.task + args.output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJI_EzPkISd9"
      },
      "source": [
        "# initialize logging\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format= \"%(asctime)s - %(message)s\",\n",
        "    # format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUZThiu_893"
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# set seed\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "#TODO: log versioning is not yet done\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7j75Y0oGasc"
      },
      "source": [
        "# make Input Example\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, review_token, label):\n",
        "        self.review_token = review_token\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "        \n",
        "# make Input Feature\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyPH-5ErasPI"
      },
      "source": [
        "if args.task == 'sentiment':\n",
        "    args.logging_steps, args.save_steps = 63, 63\n",
        "    args.max_seq_len = 64\n",
        "    args.train_batch_size = 128\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def load_dataset(args, tokenizer, mode):\n",
        "    features = get_features(args, tokenizer, mode)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor(\n",
        "        [f.input_ids for f in features], \n",
        "        dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor(\n",
        "        [f.attention_mask for f in features],\n",
        "         dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor(\n",
        "        [f.token_type_ids for f in features],\n",
        "        dtype=torch.long)\n",
        "\n",
        "    if args.task == \"classification\":\n",
        "        all_labels = torch.tensor([f.label for f in features],\n",
        "                                  dtype=torch.long)\n",
        "    else:\n",
        "        all_labels = torch.tensor([f.label for f in features],\n",
        "                                  dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids,\n",
        "                            all_attention_mask,\n",
        "                            all_token_type_ids,\n",
        "                            all_labels)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_features(args, tokenizer, mode):\n",
        "    # load data and labels using Processor\n",
        "    processor = Processor(args)\n",
        "    try:\n",
        "        examples = processor.get_examples(mode)\n",
        "    except ValueError:\n",
        "        print('possible modes: train, val, test')\n",
        "    label_list = processor.get_labels()\n",
        "\n",
        "    if args.task == 'classification':\n",
        "      labels = [int(example.label) for example in examples]\n",
        "    else:\n",
        "      labels = [float(example.label) for example in examples]\n",
        "\n",
        "    batch_encoding = tokenizer.batch_encode_plus(\n",
        "        [(example.review_token) for example in examples],\n",
        "        max_length=args.max_seq_len,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    for i in range(len(examples)):\n",
        "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "        \n",
        "        # For xlm-roberta\n",
        "        inputs[\"token_type_ids\"] = [0] * len(inputs[\"input_ids\"])  \n",
        "\n",
        "        feature = InputFeatures(**inputs, label=labels[i])\n",
        "        features.append(feature)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "class Processor(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "    def get_labels(self):\n",
        "        if self.args.task == 'classification':\n",
        "            return CLASSIFICATION_DICT.keys()\n",
        "        else:\n",
        "            return [None]\n",
        "\n",
        "    def _read_file(cls, input_file):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = []\n",
        "            for line in f:\n",
        "                lines.append(line.strip())\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines[0:]):\n",
        "            line = line.split(\"\\t\")\n",
        "            review_token = line[0]\n",
        "            label = line[1]\n",
        "            examples.append(InputExample(review_token=review_token, label=label))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, test\n",
        "        \"\"\"\n",
        "        file_to_read = None\n",
        "        if mode == \"train\":\n",
        "            file_to_read = self.args.train_file\n",
        "        elif mode == \"test\":\n",
        "            file_to_read = self.args.test_file\n",
        "\n",
        "        return self._create_examples(\n",
        "            self._read_file(os.path.join(self.args.data_dir, file_to_read)), mode\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwf-8cQ9EpgT"
      },
      "source": [
        "config = args.config.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            num_labels=17 if args.task == 'classification' else 1\n",
        "        )\n",
        "tokenizer = args.tokenizer.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    do_lower_case=args.do_lower_case\n",
        ")\n",
        "model = args.model.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "train_dataset = load_dataset(args, tokenizer, mode=\"train\")\n",
        "val_dataset = load_dataset(args, tokenizer, mode=\"test\")\n",
        "datasets = {'train': train_dataset, 'val': val_dataset}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VARfUcFjGPam"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLsq-QkPGKhs"
      },
      "source": [
        "%tensorboard --logdir='runs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZXbz67inCgG"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "import torch\n",
        "\n",
        "# TODO: acc. calculating logic impl. + figure adding logic impl.\n",
        "        \n",
        "def train(args, model, datasets):\n",
        "    train_sampler = RandomSampler(datasets['train'])\n",
        "    train_loader = DataLoader(datasets['train'],\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=args.train_batch_size)\n",
        "    val_sampler = RandomSampler(datasets['val'])\n",
        "    val_loader = DataLoader(datasets['val'],\n",
        "                            sampler=val_sampler,\n",
        "                            batch_size=args.eval_batch_size)\n",
        "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "    full_text = pd.read_csv(os.path.join(args.data_dir, 'full_text.txt'),\n",
        "                            sep='\\t')\n",
        "    \n",
        "    t_total = (\n",
        "        len(dataloaders['train']) // args.gradient_accumulation_steps\n",
        "        * args.num_train_epochs\n",
        "    )\n",
        "   \n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': args.weight_decay\n",
        "         },\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0\n",
        "         }\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                      lr=args.learning_rate,\n",
        "                      eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(t_total * args.warmup_proportion),\n",
        "        num_training_steps=t_total\n",
        "        )\n",
        "\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) \\\n",
        "    and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\")):\n",
        "        # Load optimizer and scheduler states\n",
        "        optimizer.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "            )\n",
        "        scheduler.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "            )\n",
        "\n",
        "    print(f'training started [TASK: {args.task}]')\n",
        "    global_step = 0\n",
        "    tr_loss = 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    for epoch in mb:\n",
        "        mb.main_bar.comment = f'training in progress: epoch {epoch+1}'\n",
        "        accuracies, losses = {}, {}\n",
        "        for stage, dataloader in dataloaders.items():\n",
        "            epoch_iterator = progress_bar(dataloader, parent=mb)\n",
        "            num_correct, num_items = 0, 0\n",
        "            running_loss = 0\n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                mb.child.comment = f'epoch {epoch+1} {stage}'\n",
        "                if stage == 'train':\n",
        "                    model.train()\n",
        "                    optimizer.zero_grad() # why is this not in original func?\n",
        "                else:\n",
        "                    model.eval()\n",
        "                \n",
        "                batch = tuple(t.to(args.device) for t in batch)\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"labels\": batch[3]\n",
        "                }\n",
        "                inputs[\"token_type_ids\"] = batch[2]  # we don't use this\n",
        "\n",
        "                # model output: [loss, grad_fn (NLL), logits, batches]\n",
        "                outputs = model(**inputs)\n",
        "                if args.task == 'classification':\n",
        "                    preds = torch.argmax(outputs.logits, dim=1)\n",
        "                else:\n",
        "                    preds = torch.round(outputs.logits).view(-1)\n",
        "                num_items += preds.shape[0]\n",
        "                num_correct += (inputs['labels'] == preds).sum()\n",
        "\n",
        "                loss = outputs[0]\n",
        "\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "                if stage == 'train':\n",
        "                    loss.backward()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if (step + 1) % args.gradient_accumulation_steps == 0 \\\n",
        "                or (len(dataloader) <= args.gradient_accumulation_steps \\\n",
        "                    and (step + 1) == len(dataloader)) \\\n",
        "                and stage == 'train':\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
        "                                                   args.max_grad_norm)\n",
        "\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                    # TODO: evaluate function commented out for test exec\n",
        "                    # if args.logging_steps > 0 \\\n",
        "                    # and global_step % args.logging_steps == 0:\n",
        "                        # if args.evaluate_test_during_training:\n",
        "                        #     evaluate(args, model, full_text, test_dataset,\n",
        "                        #              \"test\",\n",
        "                        #              global_step)\n",
        "\n",
        "                    # if args.save_steps > 0 \\\n",
        "                    # and global_step % args.save_steps == 0:\n",
        "                    #     # Save model checkpoint\n",
        "                    #     output_dir = os.path.join(\n",
        "                    #         args.output_dir,\n",
        "                    #         \"checkpoint-{}\".format(global_step)\n",
        "                    #         )\n",
        "                    #     if not os.path.exists(output_dir):\n",
        "                    #         os.makedirs(output_dir)\n",
        "                    #     model_to_save = (\n",
        "                    #         model.module if hasattr(model, \"module\") else model\n",
        "                    #     )\n",
        "                    #     model_to_save.save_pretrained(output_dir)\n",
        "\n",
        "                    #     torch.save(args, os.path.join(output_dir,\n",
        "                    #                                   \"training_args.bin\"))\n",
        "\n",
        "                    #     if args.save_optimizer:\n",
        "                    #         torch.save(optimizer.state_dict(),\n",
        "                    #                    os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    #         torch.save(scheduler.state_dict(),\n",
        "                    #                    os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                    break\n",
        "            # if stage == 'val':\n",
        "                # fig = plt.figure(figsize=(15, 8))\n",
        "                # plt.subplot(2, 1, 1)\n",
        "                # plt.title('Bar Chart of Labels Count and Accuracy', fontsize=15)\n",
        "                # clrs = ['r' if (x < 15) else 'b' for x in count_list]\n",
        "                # p1 = plt.bar(index, count_list,\n",
        "                #             bar_width,\n",
        "                #             color=clrs,\n",
        "                #             alpha=alpha,\n",
        "                #             label='Count')\n",
        "                # plt.ylabel('Count of Labels', fontsize=12)\n",
        "                # plt.xticks([], [])\n",
        "                # plt.legend((p1[0],), ('Count',), fontsize=10)\n",
        "                # plt.subplot(2, 1, 2)\n",
        "                # p2 = plt.bar(index + bar_width, acc_tot,\n",
        "                #             bar_width,\n",
        "                #             color=clrs,\n",
        "                #             alpha=alpha,\n",
        "                #             label='Accuracy')\n",
        "                # plt.gca().yaxis.set_major_formatter(mticker.PercentFormatter())\n",
        "                # plt.ylabel('Accuracy by Labels', fontsize=12)\n",
        "                # plt.xlabel('Label', fontsize=12)\n",
        "                # plt.xticks(index, label_lst, fontsize=10, rotation=45)\n",
        "                # plt.legend((p2[0],), ('Accuracy',), fontsize=10)\n",
        "                # plt.show()\n",
        "                    # for i in range(len(out_label_ids)):\n",
        "                    #     print(tokenizer.decode(out_ids[i]), out_label_ids[i], preds[i])\n",
        "                    # print(type(out_label_ids), type(preds))\n",
        "                    # print(out_label_ids, preds)\n",
        "                    #\n",
        "\n",
        "                # log matplotlib to tensorboard\n",
        "                # writer.add_figure('bar chart of labels count and accuracy', fig)\n",
        "            accuracies[stage] = num_correct / num_items\n",
        "            losses[stage] = running_loss / len(dataloader)\n",
        "        writer.add_scalars(main_tag='accuracy',\n",
        "                           tag_scalar_dict=accuracies,\n",
        "                           global_step=epoch+1)\n",
        "        writer.add_scalars(main_tag='loss',\n",
        "                          tag_scalar_dict=losses,\n",
        "                          global_step=epoch+1)\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            break\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-5E4fNnbURq"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "torch.cuda.memory_summary(abbreviated=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxQ2yD-kN-H"
      },
      "source": [
        "# GPU or CPU\n",
        "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "model.to(args.device)\n",
        "\n",
        "# Train\n",
        "train(args, model, datasets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghFXmNuqq7q7"
      },
      "source": [
        "def evaluate(args, model, full_text, eval_dataset, mode, global_step=None):\n",
        "    results = {}\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    if global_step != None:\n",
        "        logger.info(\"***** Running evaluation on {} dataset ({} step) *****\".format(mode, global_step))\n",
        "    else:\n",
        "        logger.info(\"***** Running evaluation on {} dataset *****\".format(mode))\n",
        "    logger.info(\"  Num examples = {}\".format(len(eval_dataset)))\n",
        "    logger.info(\"  Eval Batch size = {}\".format(args.eval_batch_size))\n",
        "    eval_loss = 0.0\n",
        "    # nb_eval_steps = 0\n",
        "    preds = None\n",
        "    preds_save = None\n",
        "    out_label_ids = None\n",
        "    out_input_ids = None\n",
        "\n",
        "    for batch in progress_bar(eval_dataloader):\n",
        "        preds_temp = None\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"labels\": batch[3]\n",
        "            }\n",
        "\n",
        "            if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
        "                inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        # nb_eval_steps += 1\n",
        "        if preds is None:\n",
        "            out_input_ids = inputs['input_ids'].detach().cpu().numpy()\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            preds_save = preds.copy()\n",
        "            # 12번째\n",
        "            # if args.task == 'sentiment':\n",
        "            #   for i in range(len(preds)):\n",
        "            #     if preds[i][0] < 0.5:\n",
        "            #       preds[i][0] = 0\n",
        "            #     else:\n",
        "            #       preds[i][0] = 1\n",
        "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "        else:\n",
        "            out_input_ids = np.append(out_input_ids, inputs['input_ids'].detach().cpu().numpy(), axis=0)\n",
        "            preds_temp = logits.detach().cpu().numpy()\n",
        "            # 12번째\n",
        "            # if args.task == 'sentiment':\n",
        "            #   for i in range(len(preds_temp)):\n",
        "            #     if preds_temp[i][0] < 0.5:\n",
        "            #       print('yes')\n",
        "            #       preds_temp[i][0] = 0\n",
        "            #     else:\n",
        "            #       preds_temp[i][0] = 1\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            preds_save = np.append(preds_save, logits.detach().cpu().numpy(), axis=0)\n",
        "            # preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        # 내가 수정한 부분\n",
        "        \n",
        "        if args.task == 'sentiment':\n",
        "            preds[:][0] = torch.round(preds[:][0])\n",
        "\n",
        "    tokenizer = args.tokenizer.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case\n",
        "    )\n",
        "    # label_dict = {'칭찬': 0, '중립': 0.5, '불만': 1}\n",
        "    if args.task == 'classification':\n",
        "        label_dict = {'중립': 0, '상담원': 1, '상담시스템': 2, '혜택': 3,\n",
        "                      '할부금융상품': 4, '카드상품': 5, '청구입금': 6, '심사/한도': 7,\n",
        "                      '생활편의서비스': 8, '상담/채널': 9, '리스렌탈상품': 10,\n",
        "                      '라이프서비스': 11, '금융상품': 12,'고객정보관리': 13,\n",
        "                      '가맹점매출/승인': 14, '삼성카드': 15, '기타': 16}\n",
        "    else:\n",
        "      label_dict = {'칭찬': 0, '불만': 1}\n",
        "    label_dict = dict((v, k) for k, v in label_dict.items())\n",
        "    df_review = []\n",
        "    # temp_review = []\n",
        "    df_label = np.vectorize(label_dict.get)(out_label_ids)\n",
        "    df_prediction = np.vectorize(label_dict.get(np.argmax(preds)))\n",
        "    for i in range(len(out_input_ids)):\n",
        "        review_list = list(out_input_ids[i])\n",
        "\n",
        "\n",
        "        # temp_review.append(str(x) for x in out_input_ids[i])\n",
        "\n",
        "\n",
        "        while 0 in review_list:\n",
        "            review_list.remove(0)\n",
        "        del review_list[0]\n",
        "        del review_list[-1]\n",
        "        df_review.append(tokenizer.decode(review_list))\n",
        "        # print(review_list, label_dict[out_label_ids[i] - 1], label_dict[np.argmax(preds[i]) - 1])\n",
        "    df_data = {'Review': df_review, 'Label': df_label, 'Prediction': df_prediction}\n",
        "    df = pd.DataFrame(df_data)\n",
        "\n",
        "    df_train_data = {'Review': full_text['Review'], 'Label': full_text['Label']}\n",
        "    df_from_train = pd.DataFrame(df_train_data)\n",
        "    # Dodged Bar Chart (with same X coordinates side by side)\n",
        "\n",
        "    bar_width = 0.35\n",
        "    alpha = 0.5\n",
        "    label_lst = list(label_dict.values())\n",
        "    index = np.arange(len(label_lst))\n",
        "    count_list, cnt_pred, cnt_label = [0 for _ in range(len(label_dict))], \\\n",
        "                                    [0 for _ in range(len(label_dict))], [0 for _ in range(len(label_dict))]\n",
        "    count_labels = df_from_train.groupby('Label', as_index=False).Review.count()\n",
        "    acc_labels = df[df['Label'] == df['Prediction']].groupby('Label').Review.count()\n",
        "    viable_label = list(count_labels['Label'])\n",
        "    for i in range(len(viable_label)):\n",
        "        count_list[viable_label[i]] = count_labels['Review'][i]\n",
        "    for validation in range(len(out_label_ids)):\n",
        "        if args.task == 'sentiment':\n",
        "          if int(out_label_ids[validation]) == int(preds[validation][0]):\n",
        "              cnt_pred[int(out_label_ids[validation])] += 100\n",
        "          cnt_label[int(out_label_ids[validation])] += 1\n",
        "        else:\n",
        "          if out_label_ids[validation] == np.argmax(preds, axis=1)[validation]:\n",
        "              cnt_pred[int(out_label_ids[validation])] += 100\n",
        "          cnt_label[int(out_label_ids[validation])] += 1\n",
        "    for i in range(len(cnt_label)):\n",
        "        if cnt_label[i] == 0:\n",
        "            cnt_label[i] = 1\n",
        "    acc_tot = np.divide(cnt_pred, cnt_label)\n",
        "    acc_tot[np.isnan(acc_tot)] = 0\n",
        "    print(count_list)\n",
        "    print(acc_tot)\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title('Bar Chart of Labels Count and Accuracy', fontsize=15)\n",
        "    clrs = ['r' if (x < 15) else 'b' for x in count_list]\n",
        "    p1 = plt.bar(index, count_list,\n",
        "                 bar_width,\n",
        "                 color=clrs,\n",
        "                 alpha=alpha,\n",
        "                 label='Count')\n",
        "    plt.ylabel('Count of Labels', fontsize=12)\n",
        "    plt.xticks([], [])\n",
        "    # plt.legend((p1[0],), ('Count',), fontsize=10)\n",
        "    plt.subplot(2, 1, 2)\n",
        "    p2 = plt.bar(index + bar_width, acc_tot,\n",
        "                 bar_width,\n",
        "                 color=clrs,\n",
        "                 alpha=alpha,\n",
        "                 label='Accuracy')\n",
        "    plt.gca().yaxis.set_major_formatter(mticker.PercentFormatter())\n",
        "    plt.ylabel('Accuracy by Labels', fontsize=12)\n",
        "    plt.xlabel('Label', fontsize=12)\n",
        "    plt.xticks(index, label_lst, fontsize=10, rotation=45)\n",
        "    # plt.legend((p2[0],), ('Accuracy',), fontsize=10)\n",
        "    # plt.show()\n",
        "        # for i in range(len(out_label_ids)):\n",
        "        #     print(tokenizer.decode(out_ids[i]), out_label_ids[i], preds[i])\n",
        "        # print(type(out_label_ids), type(preds))\n",
        "        # print(out_label_ids, preds)\n",
        "        #\n",
        "\n",
        "    # log matplotlib to tensorboard\n",
        "    writer.add_figure('bar chart of labels count and accuracy', fig)\n",
        "\n",
        "    # eval_loss = eval_loss / nb_eval_steps\n",
        "    # if output_modes[args.task] == \"classification\":\n",
        "\n",
        "\n",
        "    preds_original = preds.copy()\n",
        "\n",
        "    # 9번째\n",
        "    # preds = np.argmax(preds, axis=1)\n",
        "    if args.task == 'classification':\n",
        "      preds = np.argmax(preds, axis=1)\n",
        "    else:\n",
        "      preds = np.squeeze(preds)\n",
        "\n",
        "\n",
        "    # elif output_modes[args.task] == \"regression\":\n",
        "    #     preds = np.squeeze(preds)\n",
        "    result = compute_metrics(out_label_ids, preds)\n",
        "\n",
        "    # 틀린 항목 정리\n",
        "    check_right = out_label_ids == preds\n",
        "    df = pd.DataFrame(columns=['Review', 'Label', 'Prediction', 'Softmax'])\n",
        "    for check_result in range(len(check_right)):\n",
        "        if not check_right[check_result]:\n",
        "            review_list = list(out_input_ids[check_result])\n",
        "            while 0 in review_list:\n",
        "                review_list.remove(0)\n",
        "            del review_list[0]\n",
        "            del review_list[-1]\n",
        "            df = df.append({'Review': tokenizer.decode(review_list), \n",
        "                           'Label': label_dict[out_label_ids[check_result]], \n",
        "                           'Prediction': label_dict[preds[check_result]],\n",
        "                           'Softmax': preds_save[check_result]},\n",
        "                           ignore_index=True\n",
        "                           )\n",
        "    \n",
        "    print(df.head())\n",
        "    df.to_csv(r'analysis_ckpt_{}_{}.csv'.format(args.task, global_step), header=None, index=None, sep='\\t', mode='a')\n",
        "            # print(tokenizer.decode(review_list), 'Label: ' + str(label_dict[out_label_ids[check_result]]) + ' Prediction: ' + str(\n",
        "            #     label_dict[preds[check_result]]))\n",
        "\n",
        "\n",
        "    # numpy_data = np.array(out_label_ids, preds)\n",
        "    # df = pd.DataFrame(data=numpy_data, index=[\"row1\", \"row2\"], columns=[\"column1\", \"column2\"])\n",
        "    print(result)\n",
        "    results.update(result)\n",
        "    output_dir = os.path.join(args.output_dir, mode)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    output_eval_file = os.path.join(output_dir,\n",
        "                                    \"{}-{}.txt\".format(mode, global_step) if global_step else \"{}.txt\".format(mode))\n",
        "    with open(output_eval_file, \"w\") as f_w:\n",
        "        logger.info(\"***** Eval results on {} dataset *****\".format(mode))\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  {} = {}\".format(key, str(results[key])))\n",
        "            f_w.write(\"  {} = {}\\n\".format(key, str(results[key])))\n",
        "\n",
        "    return results\n",
        "\n",
        "def compute_metrics(labels, preds):\n",
        "    assert len(preds) == len(labels)\n",
        "\n",
        "    # TODO: need to separate tr/val acc and loss + log all info in tensorboard\n",
        "    result = {\"acc\": (labels == preds).mean()}\n",
        "    writer.add_scalar(tag='val_acc',\n",
        "                      scalar_value=result['acc'])\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyPH-5ErasPI"
      },
      "source": [
        "\n",
        "\n",
        "if task == 'sentiment':\n",
        "  args.logging_steps, args.save_steps = 63, 63\n",
        "  args.max_seq_len = 64\n",
        "  args.train_batch_size = 128\n",
        "  args.learning_rate = 8e-06\n",
        "\n",
        "processor = Processor(args)\n",
        "labels = processor.get_labels()\n",
        "\n",
        "config = args.config.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            num_labels=len(labels)\n",
        "        )\n",
        "tokenizer = args.tokenizer.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    do_lower_case=args.do_lower_case\n",
        ")\n",
        "model = args.model.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "%cd /content/\n",
        "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
        "val_dataset = load_and_cache_examples(args, tokenizer, mode='val')\n",
        "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygbv1CT5vKV3"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "# torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VARfUcFjGPam"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLsq-QkPGKhs"
      },
      "source": [
        "%tensorboard --logdir='runs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxQ2yD-kN-H"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "# GPU or CPU\n",
        "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "model.to(args.device)\n",
        "\n",
        "# Train\n",
        "global_step, tr_loss, full_text = train(args, model, train_dataset, test_dataset)\n",
        "logger.info(\" global_step = {}, average loss = {}\".format(global_step, tr_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FrIV92vp7fw"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYa33-cXnym-"
      },
      "source": [
        "def test(args, model, test_data, mode, global_step=None):\n",
        "  results = {}\n",
        "  checkpoints = list(\n",
        "              os.path.dirname(c) for c in\n",
        "              sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "          )\n",
        "  print(checkpoints)\n",
        "  checkpoint = checkpoints[-1]\n",
        "  logger.info(\"Test the following checkpoint: %s\", checkpoint)\n",
        "  global_step = checkpoint.split(\"-\")[-1]\n",
        "  model = args.model.from_pretrained(checkpoint)\n",
        "  model.to(args.device)\n",
        "\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "  # Eval!\n",
        "  if global_step != None:\n",
        "      logger.info(\"***** Running Test on {} dataset ({} step) *****\".format(mode, global_step))\n",
        "  else:\n",
        "      logger.info(\"***** Running test on {} dataset *****\".format(mode))\n",
        "  logger.info(\"  Num examples = {}\".format(len(test_data)))\n",
        "  logger.info(\"  Eval Batch size = {}\".format(args.eval_batch_size))\n",
        "  eval_loss = 0.0\n",
        "  # nb_eval_steps = 0\n",
        "  preds = None\n",
        "  preds_save = None\n",
        "  out_input_ids = None\n",
        "\n",
        "  for batch in progress_bar(test_dataloader):\n",
        "      preds_temp = None\n",
        "      model.eval()\n",
        "      batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          inputs = {\n",
        "              \"input_ids\": batch[0],\n",
        "              \"attention_mask\": batch[1],\n",
        "\n",
        "              # label 없이 할 땐 여기 주석 반전\n",
        "              \"labels\": batch[3]\n",
        "\n",
        "          }\n",
        "\n",
        "          # if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
        "          inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
        "\n",
        "          outputs = model(**inputs)\n",
        "\n",
        "          # label 없이 할 땐 여기 주석 반전\n",
        "          tmp_eval_loss, logits = outputs[:2]\n",
        "          # logits = outputs[0]\n",
        "          eval_loss += tmp_eval_loss.mean().item()\n",
        "\n",
        "      # nb_eval_steps += 1\n",
        "      if preds is None:\n",
        "          out_input_ids = inputs['input_ids'].detach().cpu().numpy()\n",
        "          preds = logits.detach().cpu().numpy()\n",
        "          preds_save = preds.copy()\n",
        "\n",
        "          # label 없이 할 땐 여기 주석 반전\n",
        "          out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "\n",
        "      else:\n",
        "          out_input_ids = np.append(out_input_ids, inputs['input_ids'].detach().cpu().numpy(), axis=0)\n",
        "          preds_temp = logits.detach().cpu().numpy()\n",
        "          preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "          preds_save = np.append(preds_save, logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "          # label 없이 할 땐 여기 주석 반전\n",
        "          out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "  if args.task == 'sentiment':\n",
        "    for i in range(len(preds)):\n",
        "      if preds[i][0] < 0.5:\n",
        "        preds[i][0] = 0\n",
        "      else:\n",
        "        preds[i][0] = 1\n",
        "  preds_original = preds.copy()\n",
        "  if args.task == 'classification':\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "  else:\n",
        "    preds = np.squeeze(preds)\n",
        "  \n",
        "  # label 없이 할 땐 return 전 까지 주석 반전\n",
        "  result = compute_metrics(out_label_ids, preds)\n",
        "  tokenizer = args.tokenizer.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      do_lower_case=args.do_lower_case\n",
        "  )\n",
        "  # label_dict = {'칭찬': 0, '중립': 0.5, '불만': 1}\n",
        "  if args.task == 'classification':\n",
        "    label_dict = {'중립': 0, '상담원': 1, '상담시스템': 2, '혜택': 3, '할부금융상품': 4,\n",
        "                '카드상품': 5, '청구입금': 6, '심사/한도': 7, '생활편의서비스': 8,\n",
        "                '상담/채널': 9, '리스렌탈상품': 10, '라이프서비스': 11, '금융상품': 12,\n",
        "                '고객정보관리': 13, '가맹점매출/승인': 14, '삼성카드': 15, '기타': 16}\n",
        "  else:\n",
        "    label_dict = {'칭찬': 0, '불만': 1}\n",
        "  label_dict = dict((v, k) for k, v in label_dict.items())\n",
        "  check_right = out_label_ids == preds\n",
        "  df = pd.DataFrame(columns=['Review', 'Label', 'Prediction', 'Softmax'])\n",
        "  for check_result in range(len(check_right)):\n",
        "      if not check_right[check_result]:\n",
        "          review_list = list(out_input_ids[check_result])\n",
        "          while 0 in review_list:\n",
        "              review_list.remove(0)\n",
        "          del review_list[0]\n",
        "          del review_list[-1]\n",
        "          df = df.append({'Review': tokenizer.decode(review_list), \n",
        "                          'Label': label_dict[out_label_ids[check_result]], \n",
        "                          'Prediction': label_dict[preds[check_result]],\n",
        "                          'Softmax': preds_save[check_result]},\n",
        "                          ignore_index=True\n",
        "                          )\n",
        "  \n",
        "  print(df.head())\n",
        "\n",
        "  result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "  results.update(result)\n",
        "  print('test accuracy :', result)\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKjpWMZ3jR2X"
      },
      "source": [
        "test_result = test(args, model, val_dataset, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kj31GrlxBpV"
      },
      "source": [
        "list_preds = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDBbGwMkn_pa"
      },
      "source": [
        "list_preds.append(test_result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKkgQ6WTolBG"
      },
      "source": [
        "print(len(list_preds[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBDAIaPmo__K"
      },
      "source": [
        "def complete_result(list_preds):\n",
        "  label_dict_classification = {'중립': 0, '상담원': 1, '상담시스템': 2, '혜택': 3, '할부금융상품': 4,\n",
        "                '카드상품': 5, '청구입금': 6, '심사/한도': 7, '생활편의서비스': 8,\n",
        "                '상담/채널': 9, '리스렌탈상품': 10, '라이프서비스': 11, '금융상품': 12,\n",
        "                '고객정보관리': 13, '가맹점매출/승인': 14, '삼성카드': 15, '기타': 16}\n",
        "  label_dict_sentiment = {'칭찬': 0, '불만': 1}\n",
        "  label_dict_classification = dict((v, k) for k, v in label_dict_classification.items())\n",
        "  label_dict_sentiment = dict((v, k) for k, v in label_dict_sentiment.items())\n",
        "  for i in label_dict_classification:\n",
        "    if i not in [0, 1, 2, 16]:\n",
        "      label_dict_classification[i] = '>삼성카드>' + label_dict_classification[i]\n",
        "    else:\n",
        "      if i == (1 or 2):\n",
        "        label_dict_classification[i] = '>고객서비스>' + label_dict_classification[i]\n",
        "      if i == 16:\n",
        "        label_dict_classification[i] = '>' + label_dict_classification[i]\n",
        "  df = pd.DataFrame(columns = ['INT'])\n",
        "  sent_result = np.vectorize(label_dict_sentiment.get)(list_preds[0])\n",
        "  # class_result = np.vectorize(label_dict_classification.get)(list_preds[1])\n",
        "  # result_arr = np.core.defchararray.add(sent_result, class_result)\n",
        "  df['INT'] = sent_result\n",
        "  print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6iBwzHpDTU"
      },
      "source": [
        "complete_result(list_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOFBaYi8S7h6"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_end_time = printt(\"Model building: Start\")\n",
        "print(_model_build_end_time - _model_build_start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZX6jC22S99-"
      },
      "source": [
        "#모델 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPEMmgXfTKf9"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_start_time = printt(\"TEST: Start\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLDvBcGMSdwg"
      },
      "source": [
        "#TODO: 해당 블럭에 테스트 수행을 위한 코드를 넣으세요. (시간측정 구간)\n",
        "#분석 파일은 tsv 파일로 제공되며, 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 =공백)으로 제공됩니다.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLsLL-vSq1D"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_end_time = printt(\"Model building: Start\")\n",
        "print(_test_end_time - _test_start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_NifWnpKw-"
      },
      "source": [
        "# 결과출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UctiMXI8UQ0Q"
      },
      "source": [
        "#TODO:해당 블럭에 테스트 결과를 파일로 저장하는 코드를 넣으세요. (시간측정 제외)\n",
        "#저장 파일은tsv 파일로 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 = 테스트 결과 도출된 양식)으로 저장\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmpdIIPmkMez"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR-CGR8LIVQW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uYU_MBjIYQr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}